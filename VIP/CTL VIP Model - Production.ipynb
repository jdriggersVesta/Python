{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c41d9f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ED-IP Prediction with Random Forests\n",
    "\n",
    "The goal of the model is to predict whether or not a member is going to have an ED or IP visit in the next 180 following the latest claim stratification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5140dc89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Necessary dependencies\n",
    "import pandas as pd\n",
    "import snowflake.connector as sf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea02a87f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Connection\n",
      "Successful DataFrame Created\n",
      "Ready for Cleaning\n"
     ]
    }
   ],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "# Define warehouse, if necessary\n",
    "warehouse = 'DEVELOPER_BASIC'\n",
    "\n",
    "# Define Database, if not defined in SQL request\n",
    "#database = 'VESTA_STAGING'\n",
    "\n",
    "# Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "# Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "try:\n",
    "    # If defining a database, uncomment code set and add database in connection parameter\n",
    "    #sql = 'use {}'.format(database)\n",
    "    #execute_query(conn,sql)\n",
    "    \n",
    "    # Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    # Query to Snowflake\n",
    "    sql = '''WITH EDIP AS ( //This is sub table for a self join\n",
    "\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"CLAIMS_REPORTING\".\"CTL_MEM_PROFILE_IP_ER_SNF\" //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    WHERE MEASURE = 'ED' or MEASURE = 'IP' \n",
    "\n",
    "    ),\n",
    "\n",
    "    EDIPTABLE AS ( //This table shows the Member ID, date start, and the number of ED/IP in the next 6 months\n",
    "\n",
    "    SELECT\n",
    "        SCORE.MEMBER_ID,\n",
    "        TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "        COUNT(DISTINCT EDIP.DOS_FROM) AS ED_IP_VISITS_IN_NEXT_6_MONTHS\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "        LEFT JOIN EDIP \n",
    "            ON SCORE.MEMBER_ID = EDIP.MEMBER_ID\n",
    "                AND EDIP.DOS_FROM > TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))\n",
    "                AND DATEDIFF(days,TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) >= 45\n",
    "                AND DATEDIFF(days, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) <= 180\n",
    "    GROUP BY SCORE.MEMBER_ID,\n",
    "        DATE_START \n",
    "\n",
    "    )\n",
    "\n",
    "SELECT\n",
    "    //TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "    //DATA_DATE_START,\n",
    "    //SCORE.MEMBER_ID,\n",
    "    CASE WHEN EDIPTABLE.ED_IP_VISITS_IN_NEXT_6_MONTHS > 0 THEN 1 ELSE 0 END as ED_IP_VISIT,\n",
    "    ACSC__COUNT,\n",
    "    ACSC__SCORE,\n",
    "    ACSC_A_FIB_AND_FLUTTER,\n",
    "    ACSC_ALCOHOL_RELATED,\n",
    "    ACSC_ANEMIA,\n",
    "    ACSC_ANGINA,\n",
    "    ACSC_ASTHMA,\n",
    "    ACSC_CELLULITIS,\n",
    "    ACSC_CONGESTIVE_HEART_FAILURE,\n",
    "    ACSC_CONSTIPATION,\n",
    "    ACSC_CONVULSION_EPILEPSY,\n",
    "    ACSC_COPD,\n",
    "    ACSC_DECUBITI_STAGE_3_,\n",
    "    ACSC_DEHYDRATION_GASTROENTERITIS,\n",
    "    ACSC_DIABETES_COMPLICATIONS,\n",
    "    ACSC_DYSPEPSIA,\n",
    "    ACSC_ENT_INFECTION,\n",
    "    ACSC_HYPERTENSION,\n",
    "    ACSC_HYPOGLYCEMIA,\n",
    "    ACSC_HYPOKALEMIA,\n",
    "    ACSC_INFLUENZA_PNEUMONIA,\n",
    "    ACSC_MIGRAINE_HEADACHE,\n",
    "    ACSC_NUTRITION_DEFICIENT,\n",
    "    ACSC_PERFORATED_BLEEDING_ULCER,\n",
    "    ACSC_PROXIMAL_FEMUR_FRACTURE,\n",
    "    ACSC_PYELONEPHRITIS,\n",
    "    ACSC_UTI,\n",
    "    ACSC_VACCINE_PREVENTABLE_DX,\n",
    "    DATEDIFF(year,DOB, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))) as AGE,\n",
    "    AMB_ACSC_COST,\n",
    "    AMB_ACSC_COUNT,\n",
    "    BH__COUNT,\n",
    "    BH__SCORE,\n",
    "    BH_ALTERED_MENTAL_STATE,\n",
    "    BH_ALZHEIMERS_DEMENTIA,\n",
    "    BH_ANXIETY,\n",
    "    BH_BI_POLAR,\n",
    "    BH_DEPRESSION,\n",
    "    BH_SCHIZOPHRENIA,\n",
    "    BH_SUBABUSE,\n",
    "    CRN__COUNT,\n",
    "    CRN_AFIB,\n",
    "    CRN_ASTHMA,\n",
    "    CRN_CARDIOVASCULAR_DX,\n",
    "    CRN_CHRONIC_KIDNEY_DISEASE,\n",
    "    CRN_CONGESTIVE_HEART_FAILURE,\n",
    "    CRN_COPD,\n",
    "    CRN_DIABETES_W__ACUTE_COMP,\n",
    "    CRN_DIABETES_W__CHRONIC_COMP,\n",
    "    CRN_DIABETES_W_OUT_COMP,\n",
    "    CRN_FALLS,\n",
    "    CRN_GASTRO_ESOPH_REFLUX,\n",
    "    CRN_HIP_FRACTURE,\n",
    "    CRN_HTN,\n",
    "    CRN_OBESITY,\n",
    "    CRN_OSTEOPOROSIS,\n",
    "    CRN_PARKINSONS_DISEASE,\n",
    "    CRN_PRESSURE_ULCER,\n",
    "    CRN_PRIOR_MI,\n",
    "    CRN_PRIOR_STROKE,\n",
    "    CRN_SCORE,\n",
    "    CRN_SLEEP_APNEA,\n",
    "    CRN_SMOKING,\n",
    "    CRN_UTI,\n",
    "    DYAD_CKD_DD,\n",
    "    DYAD_CKD_OP,\n",
    "    DYAD_COPD_DD,\n",
    "    DYAD_COPD_HF,\n",
    "    DYAD_COPD_OP,\n",
    "    DYAD_COUNT,\n",
    "    DYAD_DM_CKD,\n",
    "    DYAD_DM_OP,\n",
    "    DYAD_HBP_HF,\n",
    "    DYAD_HF_CKD,\n",
    "    ED_ACSC_COST,\n",
    "    ED_ACSC_COUNT,\n",
    "    GENDER,\n",
    "    CASE WHEN \"GROUP\" = 'E' THEN 'A' ELSE \"GROUP\" END as \"GROUP\",\n",
    "    HMKR_ACSC_COST,\n",
    "    HMKR_ACSC_COUNT,\n",
    "    HTI_RISK_SCORE_V2_1,\n",
    "    IP_ACSC_COST,\n",
    "    IP_ACSC_COUNT,\n",
    "    IP_READMIT_ACSC_COST,\n",
    "    IP_READMIT_ACSC_COUNT,\n",
    "    IP_RHB_ACSC_COST,\n",
    "    IP_RHB_ACSC_COUNT,\n",
    "    CASE WHEN LANGUAGE_SPOKEN is NULL THEN 'Unknown'\n",
    "         WHEN LANGUAGE_SPOKEN = 'English' THEN 'English'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Chinese' THEN 'Chinese'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Spanish' THEN 'Spanish'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Russian' THEN 'Russian'\n",
    "    ELSE 'Other' END AS LANGUAGE_SPOKEN_CLEAN,\n",
    "    NI_COST_DENT,\n",
    "    NI_COST_ED,\n",
    "    NI_COST_HM,\n",
    "    NI_COST_HMKR,\n",
    "    NI_COST_HS,\n",
    "    NI_COST_IP,\n",
    "    NI_COST_IP_RHB,\n",
    "    NI_COST_OP,\n",
    "    NI_COST_OTH,\n",
    "    NI_COST_PCA_T1020,\n",
    "    NI_COST_PCA_T1019,\n",
    "    NI_COST_PR,\n",
    "    NI_COST_PSYC,\n",
    "    NI_COST_RX,\n",
    "    NI_COUNT_DENT,\n",
    "    NI_COUNT_ED,\n",
    "    NI_COUNT_HM,\n",
    "    NI_COUNT_HMKR,\n",
    "    NI_COUNT_HS,\n",
    "    NI_COUNT_IP,\n",
    "    NI_COUNT_IP_RHB,\n",
    "    NI_COUNT_OP,\n",
    "    NI_COUNT_OTH,\n",
    "    NI_COUNT_PCA_T1020,\n",
    "    NI_COUNT_PCA_T1019,\n",
    "    NI_COUNT_PR,\n",
    "    NI_COUNT_PSYC,\n",
    "    NI_COUNT_RX,\n",
    "    NON_IMPACTABLE_CLAIM_COUNT,\n",
    "    OP_ACSC_COST,\n",
    "    OP_ACSC_COUNT,\n",
    "    CAST(PART_C_RISK_SCORE as FLOAT) as PART_C_RISK_SCORE,\n",
    "    PCA_T1020_ACSC_COUNT,\n",
    "    PCA_T1020_ACSC_COST,\n",
    "    PCA_T1019_ACSC_COUNT,\n",
    "    PCA_T1019_ACSC_COST,\n",
    "    PR_ACSC_COST,\n",
    "    PR_ACSC_COUNT,\n",
    "    //CASE WHEN RC is NULL THEN 'UNDEFINED' ELSE RC END AS RC_CLEAN,\n",
    "    SNF_COST,\n",
    "    SNF_COUNT,\n",
    "    TOTAL_IMPACTABLE_COST,\n",
    "    TOTAL_IMPACTABLE_COST_PRO,\n",
    "    TOTAL_NON_IMPACTABLE_COST\n",
    "FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "    LEFT JOIN EDIPTABLE\n",
    "        ON SCORE.MEMBER_ID = EDIPTABLE.MEMBER_ID\n",
    "            AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) = EDIPTABLE.DATE_START\n",
    "WHERE SCORE.CLNT = 'CTL' //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) \n",
    "     between '2020-04-01' and TO_DATE(CONCAT(LEFT(CURRENT_DATE-210,7),'-01')) //This is looking at files that have had a reasonable amount of time to process'''\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # Dataframe creation\n",
    "    df = pd.DataFrame.from_records(iter(cursor), columns = [x[0] for x in cursor.description])\n",
    "    \n",
    "    print('Successful DataFrame Created')\n",
    "    \n",
    "    cursor.close\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close\n",
    "    \n",
    "print('Ready for Cleaning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   ED_IP_VISIT  ACSC__COUNT  ACSC__SCORE  ACSC_A_FIB_AND_FLUTTER  \\\n0            0          0.0          0.0                     0.0   \n1            0          0.0          0.0                     0.0   \n2            0          0.0          0.0                     0.0   \n3            0          0.0          0.0                     0.0   \n4            0          0.0          0.0                     0.0   \n\n   ACSC_ALCOHOL_RELATED  ACSC_ANEMIA  ACSC_ANGINA  ACSC_ASTHMA  \\\n0                   0.0          0.0          0.0          0.0   \n1                   0.0          0.0          0.0          0.0   \n2                   0.0          0.0          0.0          0.0   \n3                   0.0          0.0          0.0          0.0   \n4                   0.0          0.0          0.0          0.0   \n\n   ACSC_CELLULITIS  ACSC_CONGESTIVE_HEART_FAILURE  ...  PCA_T1020_ACSC_COST  \\\n0              0.0                            0.0  ...                  0.0   \n1              0.0                            0.0  ...                  0.0   \n2              0.0                            0.0  ...                  0.0   \n3              0.0                            0.0  ...                  0.0   \n4              0.0                            0.0  ...                  0.0   \n\n   PCA_T1019_ACSC_COUNT  PCA_T1019_ACSC_COST  PR_ACSC_COST  PR_ACSC_COUNT  \\\n0                   0.0                  0.0           0.0            0.0   \n1                   0.0                  0.0           0.0            0.0   \n2                   0.0                  0.0           0.0            0.0   \n3                   0.0                  0.0           0.0            0.0   \n4                   0.0                  0.0           0.0            0.0   \n\n   SNF_COST  SNF_COUNT  TOTAL_IMPACTABLE_COST  TOTAL_IMPACTABLE_COST_PRO  \\\n0       0.0        0.0                    0.0                        0.0   \n1       0.0        0.0                    0.0                        0.0   \n2       0.0        0.0                    0.0                        0.0   \n3       0.0        0.0                    0.0                        0.0   \n4       0.0        0.0                    0.0                        0.0   \n\n   TOTAL_NON_IMPACTABLE_COST  \n0               28293.040013  \n1                7053.800002  \n2               18014.679998  \n3               25961.869988  \n4               18088.559999  \n\n[5 rows x 132 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ED_IP_VISIT</th>\n      <th>ACSC__COUNT</th>\n      <th>ACSC__SCORE</th>\n      <th>ACSC_A_FIB_AND_FLUTTER</th>\n      <th>ACSC_ALCOHOL_RELATED</th>\n      <th>ACSC_ANEMIA</th>\n      <th>ACSC_ANGINA</th>\n      <th>ACSC_ASTHMA</th>\n      <th>ACSC_CELLULITIS</th>\n      <th>ACSC_CONGESTIVE_HEART_FAILURE</th>\n      <th>...</th>\n      <th>PCA_T1020_ACSC_COST</th>\n      <th>PCA_T1019_ACSC_COUNT</th>\n      <th>PCA_T1019_ACSC_COST</th>\n      <th>PR_ACSC_COST</th>\n      <th>PR_ACSC_COUNT</th>\n      <th>SNF_COST</th>\n      <th>SNF_COUNT</th>\n      <th>TOTAL_IMPACTABLE_COST</th>\n      <th>TOTAL_IMPACTABLE_COST_PRO</th>\n      <th>TOTAL_NON_IMPACTABLE_COST</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>28293.040013</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7053.800002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>18014.679998</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>25961.869988</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>18088.559999</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 132 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "95b3e1dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b10d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Find features with missing values\n",
    "sumdf = pd.DataFrame(df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "# Review features with missing values\n",
    "print('These are the features that initially had missing values within the data frame: \\n',features_to_drop,'\\n\\n')\n",
    "\n",
    "Nulls_to_correct = ['ACSC_ALCOHOL_RELATED',\n",
    "                    'ACSC_CELLULITIS',\n",
    "                    'ACSC_ENT_INFECTION',\n",
    "                    'ACSC_HYPOGLYCEMIA',\n",
    "                    'ACSC_MIGRAINE_HEADACHE',\n",
    "                    'ACSC_VACCINE_PREVENTABLE_DX',\n",
    "                    'BH_SUBABUSE',\n",
    "                    'CRN_SMOKING', \n",
    "                    'NI_COST_DENT',\n",
    "                    'NI_COST_HMKR',\n",
    "                    'NI_COST_HS',\n",
    "                    'NI_COST_IP_RHB',\n",
    "                    'NI_COST_PSYC',\n",
    "                    'NI_COUNT_DENT',\n",
    "                    'NI_COUNT_HMKR',\n",
    "                    'NI_COUNT_HS',\n",
    "                    'NI_COUNT_IP_RHB',\n",
    "                    'NI_COUNT_PSYC'] \n",
    "\n",
    "# Fill selected features with 0 value\n",
    "for col in Nulls_to_correct:\n",
    "    df[col] = df[col].fillna(0)\n",
    "    \n",
    "# Check for missing values and drop columns with missing values\n",
    "sumdf = pd.DataFrame(df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "print('These columns were dropped after because missing values were not corrected :\\n', features_to_drop,'\\n\\n')        \n",
    "df = df.drop(columns = features_to_drop)\n",
    "\n",
    "# Convert object datatypes to dummy variables\n",
    "object_list = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes == 'object':\n",
    "        object_list.append(col)\n",
    "        \n",
    "print('These are the features that were converted to dummy variables: \\n',object_list,'\\n\\n')\n",
    "df = pd.get_dummies(df, columns = object_list, drop_first = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c410a3ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option to remove low variability features\n",
    "\n",
    "This block is here to run as optional pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b57d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Drop low variability columns\n",
    "df_var = df.var()\n",
    "df.columns.to_list()\n",
    "\n",
    "features_to_drop = []\n",
    "\n",
    "for i in range(len(df.columns.to_list())):\n",
    "    #print(df.columns.to_list()[i],df_var[i])\n",
    "    if df_var[i] == 0 and df.columns.to_list()[i] != 'ED_IP_VISIT':\n",
    "        features_to_drop.append(df.columns.to_list()[i])\n",
    "\n",
    "        \n",
    "print('These are the features that were dropped because of low variability: \\n',features_to_drop,'\\n\\n')\n",
    "        \n",
    "df = df.drop(columns = features_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ad2786",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Spliting, Scaling, and SMOTE (Synthetic Minority Oversampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0da78",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data set\n",
    "X = df[[col for col in df.columns if col != 'ED_IP_VISIT']] #independent variables\n",
    "y = df[[col for col in df.columns if col == 'ED_IP_VISIT']] #dependent variable\n",
    "y = y.values.flatten()\n",
    "\n",
    "# Define MinMax Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Transform data\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30, random_state = 2)\n",
    "\n",
    "# Smote for balancing the training data set\n",
    "smote = SMOTE(random_state = 2)\n",
    "X_train,y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c9ab0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating and Training Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1567d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a Random Forest Classifier\n",
    "clf=RandomForestClassifier(n_estimators = 2000,min_samples_split = 2, min_samples_leaf = 1,\n",
    "                           max_depth = 50, bootstrap = False, n_jobs = -1,random_state = 2)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784162ce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reporting on Performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfe15d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create probabilities from the model on test data\n",
    "y_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Store probabilites in Datafram for threshold analysis\n",
    "threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "for threshold in threshold_list:\n",
    "    y_pred = [1 if result >= threshold else 0 for result in y_prob]\n",
    "   \n",
    "    #Calulating Metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    \n",
    "    #print('Thereshold: ',threshold)\n",
    "    #print(\"Accuracy: \",accuracy)\n",
    "    #print(\"Precision: \",precision)\n",
    "    #print(\"Recall: \",recall)\n",
    "    \n",
    "metric_df = pd.DataFrame()\n",
    "metric_df['Threshold'] = threshold_list\n",
    "metric_df['Accuracy'] = accuracy_list\n",
    "metric_df['Precision'] = precision_list\n",
    "metric_df['Recall'] = recall_list\n",
    "metric_df['F1'] = (2 * metric_df['Precision'] * metric_df['Recall']) / (metric_df['Precision'] + metric_df['Recall'])\n",
    "metric_df['Acc + Recall'] = metric_df['Accuracy'] + metric_df['Recall']\n",
    "\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06cf77",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Confusion Matrix for Threshold with Max Accuracy + Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c6b62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Find the max accuracy and recall from the Metric Table and corresponding Threshold\n",
    "threshold = metric_df[metric_df['Acc + Recall'] == metric_df['Acc + Recall'].max()]['Threshold'].item()\n",
    "\n",
    "y_pred = [1 if result >= threshold else 0 for result in y_prob]\n",
    "\n",
    "\n",
    "# Creating confusion maxtrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "%matplotlib inline\n",
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# Axis labels\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Calulating Metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "print('Thereshold: ',threshold)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print('F1: ', (2*precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268cee6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ROC and AUC and Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104e4ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,  y_prob)\n",
    "\n",
    "# Print(thresholds)\n",
    "auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(loc=4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Calculate precision and recall for each threshold\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# Calculate scores\n",
    "f1, auc = metrics.f1_score(y_test, y_pred), metrics.auc(recall, precision)\n",
    "\n",
    "# Plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='Random Forest')\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b258b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ensemble of Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333ca29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "forest_dict = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    start_time = time.time()\n",
    "    # Split the data set\n",
    "    X = df[[col for col in df.columns if col != 'ED_IP_VISIT']] #independent variables\n",
    "    y = df[[col for col in df.columns if col == 'ED_IP_VISIT']] #dependent variable\n",
    "    y = y.values.flatten()\n",
    "\n",
    "    # Define MinMax Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Transform data\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split X and y into training and testing sets\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30, random_state = i)\n",
    "\n",
    "    # Smote for balancing the training data set\n",
    "    smote = SMOTE(random_state = i)\n",
    "    X_train,y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Create a Random Forest Classifier\n",
    "    clf=RandomForestClassifier(n_estimators = 2000,min_samples_split = 2, min_samples_leaf = 1,\n",
    "                           max_depth = 50, bootstrap = False, n_jobs = -1,random_state = i)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    #Store Random Forest\n",
    "    forest_dict[i] = clf\n",
    "    \n",
    "    print( \"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af532778",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### New Query for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ba951",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "# Define warehouse, if necessary\n",
    "warehouse = 'DEVELOPER_BASIC'\n",
    "\n",
    "# Define Database, if not defined in SQL request\n",
    "#database = 'VESTA_STAGING'\n",
    "\n",
    "# Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "# Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "try:\n",
    "    # If defining a database, uncomment code set and add database in connection parameter\n",
    "    #sql = 'use {}'.format(database)\n",
    "    #execute_query(conn,sql)\n",
    "    \n",
    "    # Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    # Query to Snowflake\n",
    "    sql = '''\n",
    "WITH EDIP AS ( //This is sub table for a self join\n",
    "\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"CLAIMS_REPORTING\".\"CTL_MEM_PROFILE_IP_ER_SNF\" //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    WHERE MEASURE = 'ED' or MEASURE = 'IP' \n",
    "\n",
    "    ),\n",
    "\n",
    "EDIPTABLE AS ( //This table shows the Member ID, date start, and the number of ED/IP in the next 6 months\n",
    "\n",
    "    SELECT\n",
    "        SCORE.MEMBER_ID,\n",
    "        TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "        COUNT(DISTINCT EDIP.DOS_FROM) AS ED_IP_VISITS_IN_NEXT_6_MONTHS\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "        LEFT JOIN EDIP \n",
    "            ON SCORE.MEMBER_ID = EDIP.MEMBER_ID\n",
    "                AND EDIP.DOS_FROM > TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))\n",
    "                AND DATEDIFF(days,TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) >= 45\n",
    "                AND DATEDIFF(days, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) <= 180\n",
    "    GROUP BY SCORE.MEMBER_ID,\n",
    "        DATE_START \n",
    "\n",
    "    )\n",
    "\n",
    "SELECT\n",
    "    //TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "    DATA_DATE_START,\n",
    "    SCORE.MEMBER_ID,\n",
    "    CASE WHEN EDIPTABLE.ED_IP_VISITS_IN_NEXT_6_MONTHS > 0 THEN 1 ELSE 0 END as ED_IP_VISIT,\n",
    "    ACSC__COUNT,\n",
    "    ACSC__SCORE,\n",
    "    ACSC_A_FIB_AND_FLUTTER,\n",
    "    ACSC_ALCOHOL_RELATED,\n",
    "    ACSC_ANEMIA,\n",
    "    ACSC_ANGINA,\n",
    "    ACSC_ASTHMA,\n",
    "    ACSC_CELLULITIS,\n",
    "    ACSC_CONGESTIVE_HEART_FAILURE,\n",
    "    ACSC_CONSTIPATION,\n",
    "    ACSC_CONVULSION_EPILEPSY,\n",
    "    ACSC_COPD,\n",
    "    ACSC_DECUBITI_STAGE_3_,\n",
    "    ACSC_DEHYDRATION_GASTROENTERITIS,\n",
    "    ACSC_DIABETES_COMPLICATIONS,\n",
    "    ACSC_DYSPEPSIA,\n",
    "    ACSC_ENT_INFECTION,\n",
    "    ACSC_HYPERTENSION,\n",
    "    ACSC_HYPOGLYCEMIA,\n",
    "    ACSC_HYPOKALEMIA,\n",
    "    ACSC_INFLUENZA_PNEUMONIA,\n",
    "    ACSC_MIGRAINE_HEADACHE,\n",
    "    ACSC_NUTRITION_DEFICIENT,\n",
    "    ACSC_PERFORATED_BLEEDING_ULCER,\n",
    "    ACSC_PROXIMAL_FEMUR_FRACTURE,\n",
    "    ACSC_PYELONEPHRITIS,\n",
    "    ACSC_UTI,\n",
    "    ACSC_VACCINE_PREVENTABLE_DX,\n",
    "    DATEDIFF(year,DOB, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))) as AGE,\n",
    "    AMB_ACSC_COST,\n",
    "    AMB_ACSC_COUNT,\n",
    "    BH__COUNT,\n",
    "    BH__SCORE,\n",
    "    BH_ALTERED_MENTAL_STATE,\n",
    "    BH_ALZHEIMERS_DEMENTIA,\n",
    "    BH_ANXIETY,\n",
    "    BH_BI_POLAR,\n",
    "    BH_DEPRESSION,\n",
    "    BH_SCHIZOPHRENIA,\n",
    "    BH_SUBABUSE,\n",
    "    CRN__COUNT,\n",
    "    CRN_AFIB,\n",
    "    CRN_ASTHMA,\n",
    "    CRN_CARDIOVASCULAR_DX,\n",
    "    CRN_CHRONIC_KIDNEY_DISEASE,\n",
    "    CRN_CONGESTIVE_HEART_FAILURE,\n",
    "    CRN_COPD,\n",
    "    CRN_DIABETES_W__ACUTE_COMP,\n",
    "    CRN_DIABETES_W__CHRONIC_COMP,\n",
    "    CRN_DIABETES_W_OUT_COMP,\n",
    "    CRN_FALLS,\n",
    "    CRN_GASTRO_ESOPH_REFLUX,\n",
    "    CRN_HIP_FRACTURE,\n",
    "    CRN_HTN,\n",
    "    CRN_OBESITY,\n",
    "    CRN_OSTEOPOROSIS,\n",
    "    CRN_PARKINSONS_DISEASE,\n",
    "    CRN_PRESSURE_ULCER,\n",
    "    CRN_PRIOR_MI,\n",
    "    CRN_PRIOR_STROKE,\n",
    "    CRN_SCORE,\n",
    "    CRN_SLEEP_APNEA,\n",
    "    CRN_SMOKING,\n",
    "    CRN_UTI,\n",
    "    DYAD_CKD_DD,\n",
    "    DYAD_CKD_OP,\n",
    "    DYAD_COPD_DD,\n",
    "    DYAD_COPD_HF,\n",
    "    DYAD_COPD_OP,\n",
    "    DYAD_COUNT,\n",
    "    DYAD_DM_CKD,\n",
    "    DYAD_DM_OP,\n",
    "    DYAD_HBP_HF,\n",
    "    DYAD_HF_CKD,\n",
    "    ED_ACSC_COST,\n",
    "    ED_ACSC_COUNT,\n",
    "    GENDER,\n",
    "    CASE WHEN \"GROUP\" = 'E' THEN 'A' ELSE \"GROUP\" END as \"GROUP\",\n",
    "    HMKR_ACSC_COST,\n",
    "    HMKR_ACSC_COUNT,\n",
    "    HTI_RISK_SCORE_V2_1,\n",
    "    IP_ACSC_COST,\n",
    "    IP_ACSC_COUNT,\n",
    "    IP_READMIT_ACSC_COST,\n",
    "    IP_READMIT_ACSC_COUNT,\n",
    "    IP_RHB_ACSC_COST,\n",
    "    IP_RHB_ACSC_COUNT,\n",
    "    CASE WHEN LANGUAGE_SPOKEN is NULL THEN 'Unknown'\n",
    "         WHEN LANGUAGE_SPOKEN = 'English' THEN 'English'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Chinese' THEN 'Chinese'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Spanish' THEN 'Spanish'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Russian' THEN 'Russian'\n",
    "    ELSE 'Other' END AS LANGUAGE_SPOKEN_CLEAN,\n",
    "    NI_COST_DENT,\n",
    "    NI_COST_ED,\n",
    "    NI_COST_HM,\n",
    "    NI_COST_HMKR,\n",
    "    NI_COST_HS,\n",
    "    NI_COST_IP,\n",
    "    NI_COST_IP_RHB,\n",
    "    NI_COST_OP,\n",
    "    NI_COST_OTH,\n",
    "    NI_COST_PCA_T1020,\n",
    "    NI_COST_PCA_T1019,\n",
    "    NI_COST_PR,\n",
    "    NI_COST_PSYC,\n",
    "    NI_COST_RX,\n",
    "    NI_COUNT_DENT,\n",
    "    NI_COUNT_ED,\n",
    "    NI_COUNT_HM,\n",
    "    NI_COUNT_HMKR,\n",
    "    NI_COUNT_HS,\n",
    "    NI_COUNT_IP,\n",
    "    NI_COUNT_IP_RHB,\n",
    "    NI_COUNT_OP,\n",
    "    NI_COUNT_OTH,\n",
    "    NI_COUNT_PCA_T1020,\n",
    "    NI_COUNT_PCA_T1019,\n",
    "    NI_COUNT_PR,\n",
    "    NI_COUNT_PSYC,\n",
    "    NI_COUNT_RX,\n",
    "    NON_IMPACTABLE_CLAIM_COUNT,\n",
    "    OP_ACSC_COST,\n",
    "    OP_ACSC_COUNT,\n",
    "    CAST(PART_C_RISK_SCORE as FLOAT) as PART_C_RISK_SCORE,\n",
    "    PCA_T1020_ACSC_COUNT,\n",
    "    PCA_T1020_ACSC_COST,\n",
    "    PCA_T1019_ACSC_COUNT,\n",
    "    PCA_T1019_ACSC_COST,\n",
    "    PR_ACSC_COST,\n",
    "    PR_ACSC_COUNT,\n",
    "    //CASE WHEN RC is NULL THEN 'UNDEFINED' ELSE RC END AS RC_CLEAN,\n",
    "    SNF_COST,\n",
    "    SNF_COUNT,\n",
    "    TOTAL_IMPACTABLE_COST,\n",
    "    TOTAL_IMPACTABLE_COST_PRO,\n",
    "    TOTAL_NON_IMPACTABLE_COST\n",
    "FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "    LEFT JOIN EDIPTABLE\n",
    "        ON SCORE.MEMBER_ID = EDIPTABLE.MEMBER_ID\n",
    "            AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) = EDIPTABLE.DATE_START\n",
    "WHERE SCORE.CLNT = 'CTL' //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) \n",
    "        = (SELECT max(TO_DATE(CONCAT(LEFT(DATA_DATE_START,4),'-',RIGHT(DATA_DATE_START,2),'-01')))\n",
    "        FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" WHERE CLNT = 'CTL')\n",
    "        // Most Recent Stratification for Client// THIS NEEDS TO CHANGE BASED ON CLIENT'''\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # Dataframe creation\n",
    "    test_df = pd.DataFrame.from_records(iter(cursor), columns = [x[0] for x in cursor.description])\n",
    "    \n",
    "    print('Successful DataFrame Created')\n",
    "    \n",
    "    cursor.close\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close\n",
    "    \n",
    "print('Ready for Cleaning')  \n",
    "\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa4a7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225202e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Member ID and Data Start Date from Data Frame for prediction\n",
    "data_date_start_list = test_df['DATA_DATE_START']\n",
    "test_df = test_df.drop(['DATA_DATE_START'], axis = 1)\n",
    "member_id_list = test_df['MEMBER_ID']\n",
    "test_df = test_df.drop(['MEMBER_ID'], axis = 1)\n",
    "\n",
    "#Remove ED_Visit from Data Frame\n",
    "y_test_df = test_df['ED_IP_VISIT']\n",
    "test_df = test_df.drop(['ED_IP_VISIT'], axis = 1)\n",
    "\n",
    "# Find features with missing values\n",
    "sumdf = pd.DataFrame(test_df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "# Review features with missing values\n",
    "print('These are the features that initially had missing values within the data frame: \\n',features_to_drop,'\\n\\n')\n",
    "\n",
    "Nulls_to_correct = ['ACSC_ALCOHOL_RELATED',\n",
    "                    'ACSC_CELLULITIS',\n",
    "                    'ACSC_ENT_INFECTION',\n",
    "                    'ACSC_HYPOGLYCEMIA',\n",
    "                    'ACSC_MIGRAINE_HEADACHE',\n",
    "                    'ACSC_VACCINE_PREVENTABLE_DX',\n",
    "                    'BH_SUBABUSE',\n",
    "                    'CRN_SMOKING',\n",
    "                    'CRN_PRIOR_MI',\n",
    "                    'NI_COST_DENT',\n",
    "                    'NI_COST_HMKR',\n",
    "                    'NI_COST_HS',\n",
    "                    'NI_COST_IP_RHB',\n",
    "                    'NI_COST_PSYC',\n",
    "                    'NI_COUNT_DENT',\n",
    "                    'NI_COUNT_HMKR',\n",
    "                    'NI_COUNT_HS',\n",
    "                    'NI_COUNT_IP_RHB',\n",
    "                    'NI_COUNT_PSYC'] \n",
    "\n",
    "# Fill selected features with 0 value\n",
    "for col in Nulls_to_correct:\n",
    "    test_df[col] = test_df[col].fillna(0)\n",
    "    \n",
    "# Check for missing values and drop columns with missing values\n",
    "sumdf = pd.DataFrame(test_df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "print('These columns were dropped after because missing values were not corrected :\\n', features_to_drop,'\\n\\n')        \n",
    "test_df = test_df.drop(columns = features_to_drop)\n",
    "\n",
    "# Convert object datatypes to dummy variables\n",
    "object_list = []\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtypes == 'object':\n",
    "        object_list.append(col)\n",
    "        \n",
    "print('These are the features that were converted to dummy variables: \\n',object_list,'\\n\\n')\n",
    "test_df = pd.get_dummies(test_df, columns = object_list, drop_first = True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb77d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_to_drop = ['HMKR_ACSC_COST', 'HMKR_ACSC_COUNT', 'IP_RHB_ACSC_COST', \n",
    "                    'IP_RHB_ACSC_COUNT', 'NI_COST_DENT', 'NI_COST_HS', 'NI_COST_IP_RHB', \n",
    "                    'NI_COST_PSYC', 'NI_COUNT_DENT', 'NI_COUNT_HS', 'NI_COUNT_IP_RHB', 'NI_COUNT_PSYC','PART_C_RISK_SCORE']\n",
    "\n",
    " \n",
    "test_df = test_df.drop(columns = features_to_drop)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d667e37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Scale data for prediction with original model\n",
    "X = test_df\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13063e2d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Create Data Frame to store results\n",
    "model_df = pd.DataFrame()\n",
    "\n",
    "for i,clf in forest_dict.items():\n",
    "    start_time = time.time()\n",
    "    np.random.seed(i)\n",
    "    model_df[i+1] = clf.predict_proba(X)[:,1]\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "model_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b11203",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_df['VIP_SCORE'] = (model_df[1] + model_df[2] + model_df[3] + model_df[4] + \n",
    "    model_df[5] +model_df[6] + model_df[7] + model_df[8] + model_df[9] + model_df[10])/10\n",
    "\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a688c58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Merge Results back to Test Data Frame\n",
    "test_df['VIP_SCORE'] = model_df['VIP_SCORE']\n",
    "test_df['ED_IP_VISIT'] = y_test_df\n",
    "test_df['MEMBER_ID'] = member_id_list\n",
    "test_df['DATA_DATE_START'] = data_date_start_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ae136",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#View Predicted Members DF\n",
    "predicted_df = test_df[['MEMBER_ID','VIP_SCORE','ED_IP_VISIT','DATA_DATE_START']]\n",
    "predicted_df = predicted_df.sort_values(by=['VIP_SCORE'],ascending = False)\n",
    "predicted_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246c5ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted_df.to_csv('CTL Model - 202208.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17587d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Output and Update Tables in Snowflake for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144d0e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "\n",
    "#Define parameters if neccessary\n",
    "warehouse = os.getenv('Snowflake_warehouse')\n",
    "database = os.getenv('Snowflake_database')\n",
    "schema = os.getenv('Snowflake_schema')\n",
    "\n",
    "#Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "#Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "try:\n",
    "    sql = 'USE DATABASE {}'.format(database)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    sql = 'USE SCHEMA {}.{}'.format(database,schema)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    #Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    #Query to Snowflake\n",
    "    sql = \"CREATE TABLE IF NOT EXISTS VIP_SCORING (CLNT string,MEMBER_ID string, VIP_SCORE float ,DATA_DATE_START string)\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    entry_list = []\n",
    "    for row in predicted_df.iterrows():\n",
    "        client = 'CTL'\n",
    "        member_id = str(row[1][0])\n",
    "        vip_score = row[1][1]\n",
    "        data_date = str(row[1][3])\n",
    "        entry = (client,member_id,vip_score,data_date)\n",
    "        entry_list.append(entry)       \n",
    "    entry = str(entry_list)[1:len(str(entry_list))-1]\n",
    "    sql = 'INSERT INTO VIP_SCORING (CLNT,MEMBER_ID, VIP_SCORE, DATA_DATE_START) VALUES {}'.format(entry)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)  \n",
    "    cursor.close\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7003b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ITEMS BELOW THIS ARE FOR ANALYSIS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1182c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Prediction List\n",
    "for row in predicted_df.iterrows():\n",
    "    print('Member: ',row[1][0],' VIP_SCORE: ',round(row[1][1],4), ' ED_IP_VISIT: ',row[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc46052",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_threshold = 0.3\n",
    "\n",
    "final_y_pred = [1 if result >= final_threshold else 0 for result in test_df['VIP_SCORE'].tolist()]\n",
    "\n",
    "# Creating confusion maxtrix\n",
    "new_cnf_matrix = metrics.confusion_matrix(y_test_df,final_y_pred)\n",
    "\n",
    "%matplotlib inline\n",
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(new_cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# Axis labels\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#Calulating Metrics\n",
    "accuracy = metrics.accuracy_score(y_test_df, final_y_pred)\n",
    "precision = metrics.precision_score(y_test_df, final_y_pred)\n",
    "recall = metrics.recall_score(y_test_df, final_y_pred)\n",
    "    \n",
    "print('Thereshold: ',final_threshold)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fe52b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Realtime Accuracy\n",
    "X = []\n",
    "Y = []\n",
    "count = 1\n",
    "sums = 0\n",
    "for row in predicted_df.iterrows():\n",
    "    sums += row[1][2]\n",
    "    accuracy = round(sums/count *100,2)\n",
    "    X.append(count)\n",
    "    Y.append(accuracy)\n",
    "    print('After',count, 'prediction, the realtime accuarcy is ',accuracy)\n",
    "    count +=1\n",
    "    \n",
    "plt.plot(X, Y, marker='.', label='Random Forest')\n",
    "\n",
    "acc = np.where(np.array(Y) <= 60)\n",
    "\n",
    "acc = np.where(np.array(Y) <= 60)\n",
    "#for i,x in enumerate(acc[0]):\n",
    "    #print(i,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f997c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe6e06",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_list = [x for x in df.columns if x != 'ED_IP_VISIT']\n",
    "\n",
    "importance_df = pd.DataFrame()\n",
    "\n",
    "for i,clf in forest_dict.items():\n",
    "    start_time = time.time()\n",
    "    importances = list(clf.feature_importances_)\n",
    "    feature_importances = [(feature, round(importance, 15)) for feature, importance in zip(feature_list, importances)]\n",
    "    importance_df[i+1] = feature_importances\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "importance_avg_list = []\n",
    "\n",
    "for row in importance_df.iterrows():\n",
    "    feature_name = row[1][1][0]\n",
    "    avg = round((row[1][1][1] + row[1][2][1] + row[1][3][1] + row[1][4][1] + row[1][5][1] + row[1][6][1] + row[1][7][1] \n",
    "            + row[1][8][1] + row[1][9][1] + row[1][10][1])/10,4)\n",
    "    \n",
    "    importance_avg_list.append((feature_name,avg))\n",
    "    \n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(importance_avg_list, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "feature_importances[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a47259",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list1 = df.columns.tolist()\n",
    "list2 = test_df.columns.tolist()\n",
    "list3 = set(list2)\n",
    "[item for item in list1 if item not in list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46952730",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}