{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f43ec2",
   "metadata": {},
   "source": [
    "# ED-IP Prediction with Random Forests\n",
    "\n",
    "The goal of the model is to predict whether or not a member is going to have an ED or IP visit in the next 180 following the latest claim stratification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23059ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary dependencies\n",
    "import pandas as pd,datetime\n",
    "import snowflake.connector as sf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31578cb0",
   "metadata": {},
   "source": [
    "### Extracting Data Frame from Snowflake Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Connection\n",
      "Successful DataFrame Created\n",
      "Ready for Cleaning\n"
     ]
    }
   ],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "# Define warehouse, if neccessary\n",
    "warehouse = 'DEVELOPER_BASIC'\n",
    "\n",
    "# Define Database, if not defined in SQL request\n",
    "#database = 'VESTA_STAGING'\n",
    "\n",
    "# Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "# Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "try:\n",
    "    # If defining a database, uncomment code set and add database in connection parameter\n",
    "    #sql = 'use {}'.format(database)\n",
    "    #execute_query(conn,sql)1011\n",
    "    \n",
    "    # Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    # Query to Snowflake\n",
    "    sql = '''WITH EDIP AS ( //This is sub table for a self join\n",
    "\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"CLAIMS_REPORTING\".\"CCA_MEM_PROFILE_IP_ER_SNF\" //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    WHERE MEASURE = 'ED' or MEASURE = 'IP' \n",
    "\n",
    "    ),\n",
    "\n",
    "    EDIPTABLE AS ( //This table shows the Member ID, date start, and the number of ED/IP in the next 6 months\n",
    "\n",
    "    SELECT\n",
    "        SCORE.MEMBER_ID,\n",
    "        TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "        COUNT(DISTINCT EDIP.DOS_FROM) AS ED_IP_VISITS_IN_NEXT_6_MONTHS\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "        LEFT JOIN EDIP \n",
    "            ON SCORE.MEMBER_ID = EDIP.MEMBER_ID\n",
    "                AND EDIP.DOS_FROM > TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))\n",
    "                AND DATEDIFF(days,TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) >= 45\n",
    "                AND DATEDIFF(days, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) <= 180\n",
    "    GROUP BY SCORE.MEMBER_ID,\n",
    "        DATE_START \n",
    "\n",
    "    )\n",
    "\n",
    "SELECT\n",
    "    //TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "    //DATA_DATE_START,\n",
    "    //SCORE.MEMBER_ID,\n",
    "    CASE WHEN EDIPTABLE.ED_IP_VISITS_IN_NEXT_6_MONTHS > 0 THEN 1 ELSE 0 END as ED_IP_VISIT,\n",
    "    ACSC__COUNT,\n",
    "    ACSC__SCORE,\n",
    "    ACSC_A_FIB_AND_FLUTTER,\n",
    "    ACSC_ALCOHOL_RELATED,\n",
    "    ACSC_ANEMIA,\n",
    "    ACSC_ANGINA,\n",
    "    ACSC_ASTHMA,\n",
    "    ACSC_CELLULITIS,\n",
    "    ACSC_CONGESTIVE_HEART_FAILURE,\n",
    "    ACSC_CONSTIPATION,\n",
    "    ACSC_CONVULSION_EPILEPSY,\n",
    "    ACSC_COPD,\n",
    "    ACSC_DECUBITI_STAGE_3_,\n",
    "    ACSC_DEHYDRATION_GASTROENTERITIS,\n",
    "    ACSC_DIABETES_COMPLICATIONS,\n",
    "    ACSC_DYSPEPSIA,\n",
    "    ACSC_ENT_INFECTION,\n",
    "    ACSC_HYPERTENSION,\n",
    "    ACSC_HYPOGLYCEMIA,\n",
    "    ACSC_HYPOKALEMIA,\n",
    "    ACSC_INFLUENZA_PNEUMONIA,\n",
    "    ACSC_MIGRAINE_HEADACHE,\n",
    "    ACSC_NUTRITION_DEFICIENT,\n",
    "    ACSC_PERFORATED_BLEEDING_ULCER,\n",
    "    ACSC_PROXIMAL_FEMUR_FRACTURE,\n",
    "    ACSC_PYELONEPHRITIS,\n",
    "    ACSC_UTI,\n",
    "    ACSC_VACCINE_PREVENTABLE_DX,\n",
    "    DATEDIFF(year,DOB, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))) as AGE,\n",
    "    AMB_ACSC_COST,\n",
    "    AMB_ACSC_COUNT,\n",
    "    BH__COUNT,\n",
    "    BH__SCORE,\n",
    "    BH_ALTERED_MENTAL_STATE,\n",
    "    BH_ALZHEIMERS_DEMENTIA,\n",
    "    BH_ANXIETY,\n",
    "    BH_BI_POLAR,\n",
    "    BH_DEPRESSION,\n",
    "    BH_SCHIZOPHRENIA,\n",
    "    BH_SUBABUSE,\n",
    "    CRN__COUNT,\n",
    "    CRN_AFIB,\n",
    "    CRN_ASTHMA,\n",
    "    CRN_CARDIOVASCULAR_DX,\n",
    "    CRN_CHRONIC_KIDNEY_DISEASE,\n",
    "    CRN_CONGESTIVE_HEART_FAILURE,\n",
    "    CRN_COPD,\n",
    "    CRN_DIABETES_W__ACUTE_COMP,\n",
    "    CRN_DIABETES_W__CHRONIC_COMP,\n",
    "    CRN_DIABETES_W_OUT_COMP,\n",
    "    CRN_FALLS,\n",
    "    CRN_GASTRO_ESOPH_REFLUX,\n",
    "    CRN_HIP_FRACTURE,\n",
    "    CRN_HTN,\n",
    "    CRN_OBESITY,\n",
    "    CRN_OSTEOPOROSIS,\n",
    "    CRN_PARKINSONS_DISEASE,\n",
    "    CRN_PRESSURE_ULCER,\n",
    "    CRN_PRIOR_MI,\n",
    "    CRN_PRIOR_STROKE,\n",
    "    CRN_SCORE,\n",
    "    CRN_SLEEP_APNEA,\n",
    "    CRN_SMOKING,\n",
    "    CRN_UTI,\n",
    "    DYAD_CKD_DD,\n",
    "    DYAD_CKD_OP,\n",
    "    DYAD_COPD_DD,\n",
    "    DYAD_COPD_HF,\n",
    "    DYAD_COPD_OP,\n",
    "    DYAD_COUNT,\n",
    "    DYAD_DM_CKD,\n",
    "    DYAD_DM_OP,\n",
    "    DYAD_HBP_HF,\n",
    "    DYAD_HF_CKD,\n",
    "    ED_ACSC_COST,\n",
    "    ED_ACSC_COUNT,\n",
    "    GENDER,\n",
    "    CASE WHEN \"GROUP\" = 'E' THEN 'A' ELSE \"GROUP\" END as \"GROUP\",\n",
    "    HMKR_ACSC_COST,\n",
    "    HMKR_ACSC_COUNT,\n",
    "    HTI_RISK_SCORE_V2_1,\n",
    "    IP_ACSC_COST,\n",
    "    IP_ACSC_COUNT,\n",
    "    IP_READMIT_ACSC_COST,\n",
    "    IP_READMIT_ACSC_COUNT,\n",
    "    IP_RHB_ACSC_COST,\n",
    "    IP_RHB_ACSC_COUNT,\n",
    "    CASE WHEN LANGUAGE_SPOKEN is NULL THEN 'Unknown'\n",
    "         WHEN LANGUAGE_SPOKEN = 'English' THEN 'English'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Chinese' THEN 'Chinese'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Spanish' THEN 'Spanish'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Russian' THEN 'Russian'\n",
    "    ELSE 'Other' END AS LANGUAGE_SPOKEN_CLEAN,\n",
    "    NI_COST_DENT,\n",
    "    NI_COST_ED,\n",
    "    NI_COST_HM,\n",
    "    NI_COST_HMKR,\n",
    "    NI_COST_HS,\n",
    "    NI_COST_IP,\n",
    "    NI_COST_IP_RHB,\n",
    "    NI_COST_OP,\n",
    "    NI_COST_OTH,\n",
    "    NI_COST_PCA_T1020,\n",
    "    NI_COST_PCA_T1019,\n",
    "    NI_COST_PR,\n",
    "    NI_COST_PSYC,\n",
    "    NI_COST_RX,\n",
    "    NI_COUNT_DENT,\n",
    "    NI_COUNT_ED,\n",
    "    NI_COUNT_HM,\n",
    "    NI_COUNT_HMKR,\n",
    "    NI_COUNT_HS,\n",
    "    NI_COUNT_IP,\n",
    "    NI_COUNT_IP_RHB,\n",
    "    NI_COUNT_OP,\n",
    "    NI_COUNT_OTH,\n",
    "    NI_COUNT_PCA_T1020,\n",
    "    NI_COUNT_PCA_T1019,\n",
    "    NI_COUNT_PR,\n",
    "    NI_COUNT_PSYC,\n",
    "    NI_COUNT_RX,\n",
    "    NON_IMPACTABLE_CLAIM_COUNT,\n",
    "    OP_ACSC_COST,\n",
    "    OP_ACSC_COUNT,\n",
    "    CAST(PART_C_RISK_SCORE as FLOAT) as PART_C_RISK_SCORE,\n",
    "    PCA_T1020_ACSC_COUNT,\n",
    "    PCA_T1020_ACSC_COST,\n",
    "    PCA_T1019_ACSC_COUNT,\n",
    "    PCA_T1019_ACSC_COST,\n",
    "    PR_ACSC_COST,\n",
    "    PR_ACSC_COUNT,\n",
    "    CASE WHEN RC is NULL THEN 'UNDEFINED' ELSE RC END AS RC_CLEAN,\n",
    "    SNF_COST,\n",
    "    SNF_COUNT,\n",
    "    TOTAL_IMPACTABLE_COST,\n",
    "    TOTAL_IMPACTABLE_COST_PRO,\n",
    "    TOTAL_NON_IMPACTABLE_COST\n",
    "FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "    LEFT JOIN EDIPTABLE\n",
    "        ON SCORE.MEMBER_ID = EDIPTABLE.MEMBER_ID\n",
    "            AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) = EDIPTABLE.DATE_START\n",
    "WHERE SCORE.CLNT = 'CCA' //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) \n",
    "    < TO_DATE(CONCAT(LEFT(CURRENT_DATE-210,7),'-01')) //This is looking at files that have had a reasonable amount of time to process'''\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # Dataframe creation\n",
    "    df = pd.DataFrame.from_records(iter(cursor), columns = [x[0] for x in cursor.description])\n",
    "    \n",
    "    print('Successful DataFrame Created')\n",
    "    \n",
    "    cursor.close\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close\n",
    "    \n",
    "print('Ready for Cleaning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cleaning Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features that initially had missing values within the data frame: \n",
      " ['ACSC_A_FIB_AND_FLUTTER', 'ACSC_ALCOHOL_RELATED', 'ACSC_ANEMIA', 'ACSC_ANGINA', 'ACSC_ASTHMA', 'ACSC_CELLULITIS', 'ACSC_CONSTIPATION', 'ACSC_CONVULSION_EPILEPSY', 'ACSC_DECUBITI_STAGE_3_', 'ACSC_DEHYDRATION_GASTROENTERITIS', 'ACSC_ENT_INFECTION', 'ACSC_HYPOGLYCEMIA', 'ACSC_HYPOKALEMIA', 'ACSC_MIGRAINE_HEADACHE', 'ACSC_PERFORATED_BLEEDING_ULCER', 'ACSC_PROXIMAL_FEMUR_FRACTURE', 'ACSC_PYELONEPHRITIS', 'ACSC_VACCINE_PREVENTABLE_DX', 'CRN_PRIOR_MI', 'CRN_SMOKING', 'NI_COST_HS', 'NI_COST_IP_RHB', 'NI_COST_PSYC', 'NI_COUNT_HS', 'NI_COUNT_IP_RHB', 'NI_COUNT_PSYC'] \n",
      "\n",
      "\n",
      "These columns were dropped after because missing values were not corrected :\n",
      " [] \n",
      "\n",
      "\n",
      "These are the features that were converted to dummy variables: \n",
      " ['GENDER', 'GROUP', 'LANGUAGE_SPOKEN_CLEAN', 'RC_CLEAN'] \n",
      "\n",
      "\n",
      "ED_IP_VISIT                        int64\n",
      "ACSC__COUNT                      float64\n",
      "ACSC__SCORE                      float64\n",
      "ACSC_A_FIB_AND_FLUTTER           float64\n",
      "ACSC_ALCOHOL_RELATED             float64\n",
      "                                  ...   \n",
      "LANGUAGE_SPOKEN_CLEAN_Spanish      uint8\n",
      "LANGUAGE_SPOKEN_CLEAN_Unknown      uint8\n",
      "RC_CLEAN_Institutional             uint8\n",
      "RC_CLEAN_NHC                       uint8\n",
      "RC_CLEAN_Other                     uint8\n",
      "Length: 141, dtype: object\n",
      "Memory Usage of Dataframe: 84279116 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": "   ED_IP_VISIT  ACSC__COUNT  ACSC__SCORE  ACSC_A_FIB_AND_FLUTTER  \\\n0            0          0.0          0.0                     0.0   \n1            0          0.0          0.0                     0.0   \n2            0          0.0          0.0                     0.0   \n3            0          0.0          0.0                     0.0   \n4            0          0.0          0.0                     0.0   \n\n   ACSC_ALCOHOL_RELATED  ACSC_ANEMIA  ACSC_ANGINA  ACSC_ASTHMA  \\\n0                   0.0          0.0          0.0          0.0   \n1                   0.0          0.0          0.0          0.0   \n2                   0.0          0.0          0.0          0.0   \n3                   0.0          0.0          0.0          0.0   \n4                   0.0          0.0          0.0          0.0   \n\n   ACSC_CELLULITIS  ACSC_CONGESTIVE_HEART_FAILURE  ...  GROUP_B  GROUP_C  \\\n0              0.0                            0.0  ...        0        0   \n1              0.0                            0.0  ...        0        0   \n2              0.0                            0.0  ...        0        0   \n3              0.0                            0.0  ...        0        0   \n4              0.0                            0.0  ...        0        0   \n\n   GROUP_D  LANGUAGE_SPOKEN_CLEAN_Other  LANGUAGE_SPOKEN_CLEAN_Russian  \\\n0        1                            0                              0   \n1        1                            1                              0   \n2        1                            1                              0   \n3        1                            0                              0   \n4        1                            0                              0   \n\n   LANGUAGE_SPOKEN_CLEAN_Spanish  LANGUAGE_SPOKEN_CLEAN_Unknown  \\\n0                              1                              0   \n1                              0                              0   \n2                              0                              0   \n3                              0                              0   \n4                              1                              0   \n\n   RC_CLEAN_Institutional  RC_CLEAN_NHC  RC_CLEAN_Other  \n0                       0             1               0  \n1                       0             1               0  \n2                       0             1               0  \n3                       0             1               0  \n4                       0             1               0  \n\n[5 rows x 141 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ED_IP_VISIT</th>\n      <th>ACSC__COUNT</th>\n      <th>ACSC__SCORE</th>\n      <th>ACSC_A_FIB_AND_FLUTTER</th>\n      <th>ACSC_ALCOHOL_RELATED</th>\n      <th>ACSC_ANEMIA</th>\n      <th>ACSC_ANGINA</th>\n      <th>ACSC_ASTHMA</th>\n      <th>ACSC_CELLULITIS</th>\n      <th>ACSC_CONGESTIVE_HEART_FAILURE</th>\n      <th>...</th>\n      <th>GROUP_B</th>\n      <th>GROUP_C</th>\n      <th>GROUP_D</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Other</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Russian</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Spanish</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Unknown</th>\n      <th>RC_CLEAN_Institutional</th>\n      <th>RC_CLEAN_NHC</th>\n      <th>RC_CLEAN_Other</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 141 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find features with missing values\n",
    "sumdf = pd.DataFrame(df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "# Review features with missing values\n",
    "print('These are the features that initially had missing values within the data frame: \\n',features_to_drop,'\\n\\n')\n",
    "\n",
    "Nulls_to_correct = ['ACSC_A_FIB_AND_FLUTTER', 'ACSC_ALCOHOL_RELATED', 'ACSC_ANEMIA', 'ACSC_ANGINA', \n",
    "                    'ACSC_ASTHMA', 'ACSC_CELLULITIS', 'ACSC_CONSTIPATION', 'ACSC_CONVULSION_EPILEPSY', \n",
    "                    'ACSC_DECUBITI_STAGE_3_', 'ACSC_DEHYDRATION_GASTROENTERITIS', 'ACSC_ENT_INFECTION', \n",
    "                    'ACSC_HYPOGLYCEMIA', 'ACSC_HYPOKALEMIA', 'ACSC_MIGRAINE_HEADACHE', 'ACSC_PERFORATED_BLEEDING_ULCER', \n",
    "                    'ACSC_PROXIMAL_FEMUR_FRACTURE', 'ACSC_PYELONEPHRITIS', 'ACSC_VACCINE_PREVENTABLE_DX', 'BH_SUBABUSE', \n",
    "                    'CRN_PRIOR_MI', 'CRN_SMOKING', 'NI_COST_DENT', 'NI_COST_HMKR', 'NI_COST_HS', 'NI_COST_IP_RHB', \n",
    "                    'NI_COST_PSYC', 'NI_COUNT_DENT', 'NI_COUNT_HMKR', 'NI_COUNT_HS', 'NI_COUNT_IP_RHB', 'NI_COUNT_PSYC'] \n",
    "\n",
    "# Fill selected features with 0 value\n",
    "for col in Nulls_to_correct:\n",
    "    df[col] = df[col].fillna(0)\n",
    "    \n",
    "# Check for missing values and drop columns with missing values\n",
    "sumdf = pd.DataFrame(df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "print('These columns were dropped after because missing values were not corrected :\\n', features_to_drop,'\\n\\n')        \n",
    "df = df.drop(columns = features_to_drop)\n",
    "\n",
    "# Convert object datatypes to dummy variables\n",
    "object_list = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes == 'object':\n",
    "        object_list.append(col)\n",
    "        \n",
    "        \n",
    "print('These are the features that were converted to dummy variables: \\n',object_list,'\\n\\n')\n",
    "df = pd.get_dummies(df, columns = object_list, drop_first = True)\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "original_memory = df.memory_usage().sum()\n",
    "print(f'Memory Usage of Dataframe: {df.memory_usage().sum()} bytes')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage of Original Dataframe: 84279116 bytes\n",
      "Memory Usage of New Dataframe: 43269800 bytes\n",
      "Memory usage reduced by:49.0%\n"
     ]
    }
   ],
   "source": [
    "#convert data types changing all float64 to float32\n",
    "df[df.select_dtypes(np.float64).columns] = df.select_dtypes(np.float64).astype(np.float32)\n",
    "new_memory = df.memory_usage().sum()\n",
    "print(f'Memory Usage of Original Dataframe: {original_memory} bytes')\n",
    "print(f'Memory Usage of New Dataframe: {new_memory} bytes')\n",
    "print(f'Memory usage reduced by:{round((original_memory-new_memory)/original_memory * 100,0)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Option to remove low variability features\n",
    "\n",
    "This block is here to run as optional pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features that were dropped because of low variability: \n",
      " [] \n",
      "\n",
      "\n",
      "Memory Usage of Dataframe: 43269800 bytes\n"
     ]
    }
   ],
   "source": [
    "# Drop low variability columns\n",
    "df_var = df.var()\n",
    "df.columns.to_list()\n",
    "\n",
    "features_to_drop = []\n",
    "\n",
    "for i in range(len(df.columns.to_list())):\n",
    "    #print(df.columns.to_list()[i],df_var[i])\n",
    "    if df_var[i] == 0 and df.columns.to_list()[i] != 'ED_IP_VISIT':\n",
    "        features_to_drop.append(df.columns.to_list()[i])\n",
    "\n",
    "        \n",
    "print('These are the features that were dropped because of low variability: \\n',features_to_drop,'\\n\\n')\n",
    "        \n",
    "df = df.drop(columns = features_to_drop)\n",
    "print(f'Memory Usage of Dataframe: {df.memory_usage().sum()} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Spliting, Scaling, and SMOTE (Synthetic Minority Oversampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data set\n",
    "X = df[[col for col in df.columns if col != 'ED_IP_VISIT']] #independent variables\n",
    "y = df[[col for col in df.columns if col == 'ED_IP_VISIT']] #dependent variable\n",
    "y = y.values.flatten()\n",
    "\n",
    "# Define MinMax Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Transform data\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30, random_state = 2)\n",
    "\n",
    "# Smote for balancing the training data set\n",
    "smote = SMOTE(random_state = 2)\n",
    "X_train,y_train = smote.fit_resample(X_train, y_train)\n",
    "print(f'Memory Usage of Dataframe: {df.memory_usage().sum()} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creating and Training Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a Random Forest Classifier\n",
    "clf=RandomForestClassifier(n_estimators = 2000,min_samples_split = 2, min_samples_leaf = 1,\n",
    "                           max_depth = 50, bootstrap = False, n_jobs = -1,random_state = 2)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train,y_train)\n",
    "print(f'Memory Usage of Dataframe: {df.memory_usage().sum()} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reporting on Performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create probabilities from the model on test data\n",
    "y_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Store probabilites in Datafram for threshold analysis\n",
    "threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "for threshold in threshold_list:\n",
    "    y_pred = [1 if result >= threshold else 0 for result in y_prob]\n",
    "   \n",
    "    #Calulating Metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    \n",
    "    #print('Thereshold: ',threshold)\n",
    "    #print(\"Accuracy: \",accuracy)\n",
    "    #print(\"Precision: \",precision)\n",
    "    #print(\"Recall: \",recall)\n",
    "    \n",
    "metric_df = pd.DataFrame()\n",
    "metric_df['Threshold'] = threshold_list\n",
    "metric_df['Accuracy'] = accuracy_list\n",
    "metric_df['Precision'] = precision_list\n",
    "metric_df['Recall'] = recall_list\n",
    "metric_df['F1'] = (2 * metric_df['Precision'] * metric_df['Recall']) / (metric_df['Precision'] + metric_df['Recall'])\n",
    "metric_df['Acc + Recall'] = metric_df['Accuracy'] + metric_df['Recall']\n",
    "\n",
    "metric_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Confusion Matrix for Threshold with Max Accuracy + Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find the max accuracy and recall from the Metric Table and corresponding Threshold\n",
    "threshold = metric_df[metric_df['Acc + Recall'] == metric_df['Acc + Recall'].max()]['Threshold'].item()\n",
    "\n",
    "y_pred = [1 if result >= threshold else 0 for result in y_prob]\n",
    "\n",
    "\n",
    "# Creating confusion maxtrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "%matplotlib inline\n",
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# Axis labels\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Calulating Metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "print('Thereshold: ',threshold)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print('F1: ', (2*precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ROC and AUC and Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,  y_prob)\n",
    "\n",
    "# Print(thresholds)\n",
    "auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(loc=4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Calculate precision and recall for each threshold\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# Calculate scores\n",
    "f1, auc = metrics.f1_score(y_test, y_pred), metrics.auc(recall, precision)\n",
    "\n",
    "# Plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='Random Forest')\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ensemble of Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 141.25446939468384 seconds ---\n",
      "--- 149.3139042854309 seconds ---\n",
      "--- 155.10966753959656 seconds ---\n",
      "--- 163.8846299648285 seconds ---\n",
      "--- 170.8333821296692 seconds ---\n",
      "--- 176.16339588165283 seconds ---\n",
      "--- 182.76911807060242 seconds ---\n",
      "--- 192.20374870300293 seconds ---\n",
      "--- 189.81468439102173 seconds ---\n",
      "--- 191.67213368415833 seconds ---\n"
     ]
    }
   ],
   "source": [
    "forest_dict = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    start_time = time.time()\n",
    "    # Split the data set\n",
    "    X = df[[col for col in df.columns if col != 'ED_IP_VISIT']] #independent variables\n",
    "    y = df[[col for col in df.columns if col == 'ED_IP_VISIT']] #dependent variable\n",
    "    y = y.values.flatten()\n",
    "\n",
    "    # Define MinMax Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Transform data\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split X and y into training and testing sets\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30, random_state = i)\n",
    "\n",
    "    # Smote for balancing the training data set\n",
    "    smote = SMOTE(random_state = i)\n",
    "    X_train,y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Create a Random Forest Classifier\n",
    "    clf=RandomForestClassifier(n_estimators = 2000,min_samples_split = 2, min_samples_leaf = 1,\n",
    "                               max_depth = 50, bootstrap = False, n_jobs = -1,random_state = i)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    #Store Random Forest\n",
    "    forest_dict[i] = clf\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ### New Query for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Connection\n",
      "Successful DataFrame Created\n",
      "Ready for Cleaning\n",
      "(2037, 135)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   DATE_START   MEMBER_ID  ED_IP_VISIT  ACSC__COUNT  ACSC__SCORE  \\\n0  2022-11-01  5365550560            0          0.0          0.0   \n1  2022-11-01  5366039898            0          0.0          0.0   \n2  2022-11-01  5365555823            0          0.0          0.0   \n3  2022-11-01  5365591632            0          0.0          0.0   \n4  2022-11-01  5365655530            0          0.0          0.0   \n\n   ACSC_A_FIB_AND_FLUTTER  ACSC_ALCOHOL_RELATED  ACSC_ANEMIA  ACSC_ANGINA  \\\n0                     0.0                   0.0          0.0          0.0   \n1                     0.0                   0.0          0.0          0.0   \n2                     0.0                   0.0          0.0          0.0   \n3                     0.0                   0.0          0.0          0.0   \n4                     0.0                   0.0          0.0          0.0   \n\n   ACSC_ASTHMA  ... PCA_T1019_ACSC_COUNT  PCA_T1019_ACSC_COST PR_ACSC_COST  \\\n0          0.0  ...                  0.0                  0.0          0.0   \n1          0.0  ...                  0.0                  0.0          0.0   \n2          0.0  ...                  0.0                  0.0          0.0   \n3          0.0  ...                  0.0                  0.0          0.0   \n4          0.0  ...                  0.0                  0.0          0.0   \n\n   PR_ACSC_COUNT  RC_CLEAN  SNF_COST  SNF_COUNT  TOTAL_IMPACTABLE_COST  \\\n0            0.0       NHC       0.0        0.0                3049.21   \n1            0.0       NHC       0.0        0.0                   0.00   \n2            0.0       NHC       0.0        0.0                   0.00   \n3            0.0       NHC       0.0        0.0                   0.00   \n4            0.0       NHC       0.0        0.0                   0.00   \n\n   TOTAL_IMPACTABLE_COST_PRO TOTAL_NON_IMPACTABLE_COST  \n0                    3049.21                  15034.33  \n1                       0.00                   6322.19  \n2                       0.00                  42688.35  \n3                       0.00                  32280.25  \n4                       0.00                   9505.96  \n\n[5 rows x 135 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DATE_START</th>\n      <th>MEMBER_ID</th>\n      <th>ED_IP_VISIT</th>\n      <th>ACSC__COUNT</th>\n      <th>ACSC__SCORE</th>\n      <th>ACSC_A_FIB_AND_FLUTTER</th>\n      <th>ACSC_ALCOHOL_RELATED</th>\n      <th>ACSC_ANEMIA</th>\n      <th>ACSC_ANGINA</th>\n      <th>ACSC_ASTHMA</th>\n      <th>...</th>\n      <th>PCA_T1019_ACSC_COUNT</th>\n      <th>PCA_T1019_ACSC_COST</th>\n      <th>PR_ACSC_COST</th>\n      <th>PR_ACSC_COUNT</th>\n      <th>RC_CLEAN</th>\n      <th>SNF_COST</th>\n      <th>SNF_COUNT</th>\n      <th>TOTAL_IMPACTABLE_COST</th>\n      <th>TOTAL_IMPACTABLE_COST_PRO</th>\n      <th>TOTAL_NON_IMPACTABLE_COST</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-11-01</td>\n      <td>5365550560</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NHC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3049.21</td>\n      <td>3049.21</td>\n      <td>15034.33</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-11-01</td>\n      <td>5366039898</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NHC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>6322.19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-11-01</td>\n      <td>5365555823</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NHC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>42688.35</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-11-01</td>\n      <td>5365591632</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NHC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>32280.25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-11-01</td>\n      <td>5365655530</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NHC</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>9505.96</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 135 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "# Define warehouse, if neccessary\n",
    "warehouse = 'DEVELOPER_BASIC'\n",
    "\n",
    "# Define Database, if not defined in SQL request\n",
    "#database = 'VESTA_STAGING'\n",
    "\n",
    "# Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "# Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "try:\n",
    "    # If defining a database, uncomment code set and add database in connection parameter\n",
    "    #sql = 'use {}'.format(database)\n",
    "    #execute_query(conn,sql)\n",
    "    \n",
    "    # Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    # Query to Snowflake\n",
    "    sql = '''\n",
    "WITH EDIP AS ( //This is sub table for a self join\n",
    "\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"CLAIMS_REPORTING\".\"CCA_MEM_PROFILE_IP_ER_SNF\" //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    WHERE MEASURE = 'ED' or MEASURE = 'IP' \n",
    "\n",
    "    ),\n",
    "\n",
    "EDIPTABLE AS ( //This table shows the Member ID, date start, and the number of ED/IP in the next 6 months\n",
    "\n",
    "    SELECT\n",
    "        SCORE.MEMBER_ID,\n",
    "        TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "        COUNT(DISTINCT EDIP.DOS_FROM) AS ED_IP_VISITS_IN_NEXT_6_MONTHS\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "        LEFT JOIN EDIP \n",
    "            ON SCORE.MEMBER_ID = EDIP.MEMBER_ID\n",
    "                AND EDIP.DOS_FROM > TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))\n",
    "                AND DATEDIFF(days,TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) >= 45\n",
    "                AND DATEDIFF(days, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) <= 180\n",
    "    GROUP BY SCORE.MEMBER_ID,\n",
    "        DATE_START \n",
    "\n",
    "    )\n",
    "\n",
    "SELECT\n",
    "    TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "    //DATA_DATE_START,\n",
    "    SCORE.MEMBER_ID,\n",
    "    CASE WHEN EDIPTABLE.ED_IP_VISITS_IN_NEXT_6_MONTHS > 0 THEN 1 ELSE 0 END as ED_IP_VISIT,\n",
    "    ACSC__COUNT,\n",
    "    ACSC__SCORE,\n",
    "    ACSC_A_FIB_AND_FLUTTER,\n",
    "    ACSC_ALCOHOL_RELATED,\n",
    "    ACSC_ANEMIA,\n",
    "    ACSC_ANGINA,\n",
    "    ACSC_ASTHMA,\n",
    "    ACSC_CELLULITIS,\n",
    "    ACSC_CONGESTIVE_HEART_FAILURE,\n",
    "    ACSC_CONSTIPATION,\n",
    "    ACSC_CONVULSION_EPILEPSY,\n",
    "    ACSC_COPD,\n",
    "    ACSC_DECUBITI_STAGE_3_,\n",
    "    ACSC_DEHYDRATION_GASTROENTERITIS,\n",
    "    ACSC_DIABETES_COMPLICATIONS,\n",
    "    ACSC_DYSPEPSIA,\n",
    "    ACSC_ENT_INFECTION,\n",
    "    ACSC_HYPERTENSION,\n",
    "    ACSC_HYPOGLYCEMIA,\n",
    "    ACSC_HYPOKALEMIA,\n",
    "    ACSC_INFLUENZA_PNEUMONIA,\n",
    "    ACSC_MIGRAINE_HEADACHE,\n",
    "    ACSC_NUTRITION_DEFICIENT,\n",
    "    ACSC_PERFORATED_BLEEDING_ULCER,\n",
    "    ACSC_PROXIMAL_FEMUR_FRACTURE,\n",
    "    ACSC_PYELONEPHRITIS,\n",
    "    ACSC_UTI,\n",
    "    ACSC_VACCINE_PREVENTABLE_DX,\n",
    "    DATEDIFF(year,DOB, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))) as AGE,\n",
    "    AMB_ACSC_COST,\n",
    "    AMB_ACSC_COUNT,\n",
    "    BH__COUNT,\n",
    "    BH__SCORE,\n",
    "    BH_ALTERED_MENTAL_STATE,\n",
    "    BH_ALZHEIMERS_DEMENTIA,\n",
    "    BH_ANXIETY,\n",
    "    BH_BI_POLAR,\n",
    "    BH_DEPRESSION,\n",
    "    BH_SCHIZOPHRENIA,\n",
    "    BH_SUBABUSE,\n",
    "    CRN__COUNT,\n",
    "    CRN_AFIB,\n",
    "    CRN_ASTHMA,\n",
    "    CRN_CARDIOVASCULAR_DX,\n",
    "    CRN_CHRONIC_KIDNEY_DISEASE,\n",
    "    CRN_CONGESTIVE_HEART_FAILURE,\n",
    "    CRN_COPD,\n",
    "    CRN_DIABETES_W__ACUTE_COMP,\n",
    "    CRN_DIABETES_W__CHRONIC_COMP,\n",
    "    CRN_DIABETES_W_OUT_COMP,\n",
    "    CRN_FALLS,\n",
    "    CRN_GASTRO_ESOPH_REFLUX,\n",
    "    CRN_HIP_FRACTURE,\n",
    "    CRN_HTN,\n",
    "    CRN_OBESITY,\n",
    "    CRN_OSTEOPOROSIS,\n",
    "    CRN_PARKINSONS_DISEASE,\n",
    "    CRN_PRESSURE_ULCER,\n",
    "    CRN_PRIOR_MI,\n",
    "    CRN_PRIOR_STROKE,\n",
    "    CRN_SCORE,\n",
    "    CRN_SLEEP_APNEA,\n",
    "    CRN_SMOKING,\n",
    "    CRN_UTI,\n",
    "    DYAD_CKD_DD,\n",
    "    DYAD_CKD_OP,\n",
    "    DYAD_COPD_DD,\n",
    "    DYAD_COPD_HF,\n",
    "    DYAD_COPD_OP,\n",
    "    DYAD_COUNT,\n",
    "    DYAD_DM_CKD,\n",
    "    DYAD_DM_OP,\n",
    "    DYAD_HBP_HF,\n",
    "    DYAD_HF_CKD,\n",
    "    ED_ACSC_COST,\n",
    "    ED_ACSC_COUNT,\n",
    "    GENDER,\n",
    "    CASE WHEN \"GROUP\" = 'E' THEN 'A' ELSE \"GROUP\" END as \"GROUP\",\n",
    "    HMKR_ACSC_COST,\n",
    "    HMKR_ACSC_COUNT,\n",
    "    HTI_RISK_SCORE_V2_1,\n",
    "    IP_ACSC_COST,\n",
    "    IP_ACSC_COUNT,\n",
    "    IP_READMIT_ACSC_COST,\n",
    "    IP_READMIT_ACSC_COUNT,\n",
    "    IP_RHB_ACSC_COST,\n",
    "    IP_RHB_ACSC_COUNT,\n",
    "    CASE WHEN LANGUAGE_SPOKEN is NULL THEN 'Unknown'\n",
    "         WHEN LANGUAGE_SPOKEN = 'English' THEN 'English'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Chinese' THEN 'Chinese'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Spanish' THEN 'Spanish'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Russian' THEN 'Russian'\n",
    "    ELSE 'Other' END AS LANGUAGE_SPOKEN_CLEAN,\n",
    "    NI_COST_DENT,\n",
    "    NI_COST_ED,\n",
    "    NI_COST_HM,\n",
    "    NI_COST_HMKR,\n",
    "    NI_COST_HS,\n",
    "    NI_COST_IP,\n",
    "    NI_COST_IP_RHB,\n",
    "    NI_COST_OP,\n",
    "    NI_COST_OTH,\n",
    "    NI_COST_PCA_T1020,\n",
    "    NI_COST_PCA_T1019,\n",
    "    NI_COST_PR,\n",
    "    NI_COST_PSYC,\n",
    "    NI_COST_RX,\n",
    "    NI_COUNT_DENT,\n",
    "    NI_COUNT_ED,\n",
    "    NI_COUNT_HM,\n",
    "    NI_COUNT_HMKR,\n",
    "    NI_COUNT_HS,\n",
    "    NI_COUNT_IP,\n",
    "    NI_COUNT_IP_RHB,\n",
    "    NI_COUNT_OP,\n",
    "    NI_COUNT_OTH,\n",
    "    NI_COUNT_PCA_T1020,\n",
    "    NI_COUNT_PCA_T1019,\n",
    "    NI_COUNT_PR,\n",
    "    NI_COUNT_PSYC,\n",
    "    NI_COUNT_RX,\n",
    "    NON_IMPACTABLE_CLAIM_COUNT,\n",
    "    OP_ACSC_COST,\n",
    "    OP_ACSC_COUNT,\n",
    "    CAST(PART_C_RISK_SCORE as FLOAT) as PART_C_RISK_SCORE,\n",
    "    PCA_T1020_ACSC_COUNT,\n",
    "    PCA_T1020_ACSC_COST,\n",
    "    PCA_T1019_ACSC_COUNT,\n",
    "    PCA_T1019_ACSC_COST,\n",
    "    PR_ACSC_COST,\n",
    "    PR_ACSC_COUNT,\n",
    "    CASE WHEN RC is NULL THEN 'UNDEFINED' ELSE RC END AS RC_CLEAN,\n",
    "    SNF_COST,\n",
    "    SNF_COUNT,\n",
    "    TOTAL_IMPACTABLE_COST,\n",
    "    TOTAL_IMPACTABLE_COST_PRO,\n",
    "    TOTAL_NON_IMPACTABLE_COST\n",
    "FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "    LEFT JOIN EDIPTABLE\n",
    "        ON SCORE.MEMBER_ID = EDIPTABLE.MEMBER_ID\n",
    "            AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) = EDIPTABLE.DATE_START\n",
    "WHERE SCORE.CLNT = 'CCA' //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) \n",
    "        = (SELECT max(TO_DATE(CONCAT(LEFT(DATA_DATE_START,4),'-',RIGHT(DATA_DATE_START,2),'-01'))) FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" WHERE CLNT = 'CCA') \n",
    "        // Most Recent Stratification for Client// THIS NEEDS TO CHANGE BASED ON CLIENT'''\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # Dataframe creation\n",
    "    test_df = pd.DataFrame.from_records(iter(cursor), columns = [x[0] for x in cursor.description])\n",
    "    \n",
    "    print('Successful DataFrame Created')\n",
    "    \n",
    "    cursor.close\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close\n",
    "    \n",
    "print('Ready for Cleaning')  \n",
    "\n",
    "print(test_df.shape)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cleaning Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features that initially had missing values within the data frame: \n",
      " ['ACSC_CELLULITIS', 'ACSC_CONSTIPATION', 'ACSC_ENT_INFECTION', 'ACSC_HYPOGLYCEMIA', 'ACSC_VACCINE_PREVENTABLE_DX', 'NI_COST_PSYC', 'NI_COUNT_PSYC'] \n",
      "\n",
      "\n",
      "These columns were dropped after because missing values were not corrected :\n",
      " [] \n",
      "\n",
      "\n",
      "These are the features that were converted to dummy variables: \n",
      " ['GENDER', 'GROUP', 'LANGUAGE_SPOKEN_CLEAN', 'RC_CLEAN'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "   ACSC__COUNT  ACSC__SCORE  ACSC_A_FIB_AND_FLUTTER  ACSC_ALCOHOL_RELATED  \\\n0          0.0          0.0                     0.0                   0.0   \n1          0.0          0.0                     0.0                   0.0   \n2          0.0          0.0                     0.0                   0.0   \n3          0.0          0.0                     0.0                   0.0   \n4          0.0          0.0                     0.0                   0.0   \n\n   ACSC_ANEMIA  ACSC_ANGINA  ACSC_ASTHMA  ACSC_CELLULITIS  \\\n0          0.0          0.0          0.0                0   \n1          0.0          0.0          0.0                0   \n2          0.0          0.0          0.0                0   \n3          0.0          0.0          0.0                0   \n4          0.0          0.0          0.0                0   \n\n   ACSC_CONGESTIVE_HEART_FAILURE  ACSC_CONSTIPATION  ...  GROUP_B  GROUP_C  \\\n0                            0.0                  0  ...        0        1   \n1                            0.0                  0  ...        0        0   \n2                            0.0                  0  ...        0        0   \n3                            0.0                  0  ...        0        0   \n4                            0.0                  0  ...        0        0   \n\n   GROUP_D  LANGUAGE_SPOKEN_CLEAN_Other  LANGUAGE_SPOKEN_CLEAN_Russian  \\\n0        0                            0                              0   \n1        0                            0                              0   \n2        0                            0                              0   \n3        0                            0                              0   \n4        0                            0                              0   \n\n   LANGUAGE_SPOKEN_CLEAN_Spanish  LANGUAGE_SPOKEN_CLEAN_Unknown  \\\n0                              1                              0   \n1                              0                              0   \n2                              0                              0   \n3                              0                              0   \n4                              0                              0   \n\n   RC_CLEAN_Institutional  RC_CLEAN_NHC  RC_CLEAN_Other  \n0                       0             1               0  \n1                       0             1               0  \n2                       0             1               0  \n3                       0             1               0  \n4                       0             1               0  \n\n[5 rows x 140 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACSC__COUNT</th>\n      <th>ACSC__SCORE</th>\n      <th>ACSC_A_FIB_AND_FLUTTER</th>\n      <th>ACSC_ALCOHOL_RELATED</th>\n      <th>ACSC_ANEMIA</th>\n      <th>ACSC_ANGINA</th>\n      <th>ACSC_ASTHMA</th>\n      <th>ACSC_CELLULITIS</th>\n      <th>ACSC_CONGESTIVE_HEART_FAILURE</th>\n      <th>ACSC_CONSTIPATION</th>\n      <th>...</th>\n      <th>GROUP_B</th>\n      <th>GROUP_C</th>\n      <th>GROUP_D</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Other</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Russian</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Spanish</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Unknown</th>\n      <th>RC_CLEAN_Institutional</th>\n      <th>RC_CLEAN_NHC</th>\n      <th>RC_CLEAN_Other</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 140 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Member ID and Data Start Date from Data Frame for prediction\n",
    "data_date_start_list = test_df['DATE_START']\n",
    "test_df = test_df.drop(['DATE_START'], axis = 1)\n",
    "member_id_list = test_df['MEMBER_ID']\n",
    "test_df = test_df.drop(['MEMBER_ID'], axis = 1)\n",
    "\n",
    "#Remove ED_Visit from Data Frame\n",
    "y_test_df = test_df['ED_IP_VISIT']\n",
    "test_df = test_df.drop(['ED_IP_VISIT'], axis = 1)\n",
    "\n",
    "# Find features with missing values\n",
    "sumdf = pd.DataFrame(test_df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "# Review features with missing values\n",
    "print('These are the features that initially had missing values within the data frame: \\n',features_to_drop,'\\n\\n')\n",
    "\n",
    "Nulls_to_correct = ['ACSC_A_FIB_AND_FLUTTER', \n",
    "                    'ACSC_ALCOHOL_RELATED',\n",
    "                    'ACSC_ANEMIA',\n",
    "                    'ACSC_ANGINA',\n",
    "                    'ACSC_ASTHMA',\n",
    "                    'ACSC_CELLULITIS',\n",
    "                    'ACSC_CONSTIPATION',\n",
    "                    'ACSC_CONVULSION_EPILEPSY',\n",
    "                    'ACSC_COPD',\n",
    "                    'ACSC_DECUBITI_STAGE_3_',\n",
    "                    'ACSC_DEHYDRATION_GASTROENTERITIS',\n",
    "                    'ACSC_DYSPEPSIA',\n",
    "                    'ACSC_ENT_INFECTION',\n",
    "                    'ACSC_HYPERTENSION',\n",
    "                    'ACSC_HYPOGLYCEMIA',\n",
    "                    'ACSC_HYPOKALEMIA',\n",
    "                    'ACSC_MIGRAINE_HEADACHE',\n",
    "                    'ACSC_NUTRITION_DEFICIENT',\n",
    "                    'ACSC_PERFORATED_BLEEDING_ULCER',\n",
    "                    'ACSC_PROXIMAL_FEMUR_FRACTURE',\n",
    "                    'ACSC_PYELONEPHRITIS',\n",
    "                    'ACSC_VACCINE_PREVENTABLE_DX',\n",
    "                    'CRN_FALLS',\n",
    "                    'CRN_PRIOR_MI',\n",
    "                    'CRN_SMOKING',\n",
    "                    'NI_COST_HS',\n",
    "                    'NI_COST_IP_RHB',\n",
    "                    'NI_COST_OTH',\n",
    "                    'NI_COST_PCA_T1020',\n",
    "                    'NI_COST_PSYC',\n",
    "                    'NI_COUNT_HS',\n",
    "                    'NI_COUNT_IP_RHB',\n",
    "                    'NI_COUNT_OTH',\n",
    "                    'NI_COUNT_PCA_T1020',\n",
    "                    'NI_COUNT_PSYC'] \n",
    "\n",
    "# Fill selected features with 0 value\n",
    "for col in Nulls_to_correct:\n",
    "    test_df[col] = test_df[col].fillna(0)\n",
    "    \n",
    "# Check for missing values and drop columns with missing values\n",
    "sumdf = pd.DataFrame(test_df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "print('These columns were dropped after because missing values were not corrected :\\n', features_to_drop,'\\n\\n')        \n",
    "test_df = test_df.drop(columns = features_to_drop)\n",
    "\n",
    "# Convert object datatypes to dummy variables\n",
    "object_list = []\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtypes == 'object':\n",
    "        object_list.append(col)\n",
    "        \n",
    "print('These are the features that were converted to dummy variables: \\n',object_list,'\\n\\n')\n",
    "test_df = pd.get_dummies(test_df, columns = object_list, drop_first = True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scale data for prediction with original model\n",
    "X = test_df\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 12.00824236869812 seconds ---\n",
      "--- 15.29495906829834 seconds ---\n",
      "--- 25.17923617362976 seconds ---\n",
      "--- 20.79357361793518 seconds ---\n",
      "--- 4.0376129150390625 seconds ---\n",
      "--- 1.0676002502441406 seconds ---\n",
      "--- 0.6540634632110596 seconds ---\n",
      "--- 1.8370513916015625 seconds ---\n",
      "--- 12.405155420303345 seconds ---\n",
      "--- 0.7447271347045898 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": "            1         2         3         4         5         6         7   \\\n0     0.255500  0.246000  0.241500  0.224500  0.259500  0.234500  0.265000   \n1     0.291179  0.350667  0.333296  0.322452  0.319961  0.332547  0.337322   \n2     0.364500  0.377500  0.423000  0.358000  0.401500  0.366500  0.408000   \n3     0.293000  0.341985  0.315349  0.325500  0.324771  0.335500  0.294357   \n4     0.311000  0.278000  0.317500  0.287000  0.290500  0.325500  0.321000   \n...        ...       ...       ...       ...       ...       ...       ...   \n2032  0.183000  0.158000  0.165000  0.170500  0.155000  0.151000  0.145417   \n2033  0.150500  0.134500  0.168000  0.147500  0.166750  0.139000  0.137000   \n2034  0.283500  0.323000  0.277750  0.300500  0.263000  0.278500  0.277667   \n2035  0.363500  0.257500  0.320000  0.307500  0.288000  0.317000  0.256375   \n2036  0.134100  0.209920  0.224571  0.189226  0.217400  0.114920  0.178167   \n\n            8         9         10  \n0     0.232500  0.262500  0.238000  \n1     0.297000  0.325634  0.289396  \n2     0.406000  0.415000  0.390500  \n3     0.333759  0.315583  0.332383  \n4     0.321000  0.319000  0.303500  \n...        ...       ...       ...  \n2032  0.196500  0.159500  0.197500  \n2033  0.169500  0.126167  0.126000  \n2034  0.229000  0.249500  0.250000  \n2035  0.240000  0.318000  0.313000  \n2036  0.234563  0.268000  0.208576  \n\n[2037 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.255500</td>\n      <td>0.246000</td>\n      <td>0.241500</td>\n      <td>0.224500</td>\n      <td>0.259500</td>\n      <td>0.234500</td>\n      <td>0.265000</td>\n      <td>0.232500</td>\n      <td>0.262500</td>\n      <td>0.238000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.291179</td>\n      <td>0.350667</td>\n      <td>0.333296</td>\n      <td>0.322452</td>\n      <td>0.319961</td>\n      <td>0.332547</td>\n      <td>0.337322</td>\n      <td>0.297000</td>\n      <td>0.325634</td>\n      <td>0.289396</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.364500</td>\n      <td>0.377500</td>\n      <td>0.423000</td>\n      <td>0.358000</td>\n      <td>0.401500</td>\n      <td>0.366500</td>\n      <td>0.408000</td>\n      <td>0.406000</td>\n      <td>0.415000</td>\n      <td>0.390500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.293000</td>\n      <td>0.341985</td>\n      <td>0.315349</td>\n      <td>0.325500</td>\n      <td>0.324771</td>\n      <td>0.335500</td>\n      <td>0.294357</td>\n      <td>0.333759</td>\n      <td>0.315583</td>\n      <td>0.332383</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.311000</td>\n      <td>0.278000</td>\n      <td>0.317500</td>\n      <td>0.287000</td>\n      <td>0.290500</td>\n      <td>0.325500</td>\n      <td>0.321000</td>\n      <td>0.321000</td>\n      <td>0.319000</td>\n      <td>0.303500</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2032</th>\n      <td>0.183000</td>\n      <td>0.158000</td>\n      <td>0.165000</td>\n      <td>0.170500</td>\n      <td>0.155000</td>\n      <td>0.151000</td>\n      <td>0.145417</td>\n      <td>0.196500</td>\n      <td>0.159500</td>\n      <td>0.197500</td>\n    </tr>\n    <tr>\n      <th>2033</th>\n      <td>0.150500</td>\n      <td>0.134500</td>\n      <td>0.168000</td>\n      <td>0.147500</td>\n      <td>0.166750</td>\n      <td>0.139000</td>\n      <td>0.137000</td>\n      <td>0.169500</td>\n      <td>0.126167</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <th>2034</th>\n      <td>0.283500</td>\n      <td>0.323000</td>\n      <td>0.277750</td>\n      <td>0.300500</td>\n      <td>0.263000</td>\n      <td>0.278500</td>\n      <td>0.277667</td>\n      <td>0.229000</td>\n      <td>0.249500</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>2035</th>\n      <td>0.363500</td>\n      <td>0.257500</td>\n      <td>0.320000</td>\n      <td>0.307500</td>\n      <td>0.288000</td>\n      <td>0.317000</td>\n      <td>0.256375</td>\n      <td>0.240000</td>\n      <td>0.318000</td>\n      <td>0.313000</td>\n    </tr>\n    <tr>\n      <th>2036</th>\n      <td>0.134100</td>\n      <td>0.209920</td>\n      <td>0.224571</td>\n      <td>0.189226</td>\n      <td>0.217400</td>\n      <td>0.114920</td>\n      <td>0.178167</td>\n      <td>0.234563</td>\n      <td>0.268000</td>\n      <td>0.208576</td>\n    </tr>\n  </tbody>\n</table>\n<p>2037 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Data Frame to store results\n",
    "model_df = pd.DataFrame()\n",
    "\n",
    "for i,clf in forest_dict.items():\n",
    "    start_time = time.time()\n",
    "    np.random.seed(i)\n",
    "    model_df[i+1] = clf.predict_proba(X)[:,1]\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "model_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             1         2         3         4         5         6         7  \\\n0     0.255500  0.246000  0.241500  0.224500  0.259500  0.234500  0.265000   \n1     0.291179  0.350667  0.333296  0.322452  0.319961  0.332547  0.337322   \n2     0.364500  0.377500  0.423000  0.358000  0.401500  0.366500  0.408000   \n3     0.293000  0.341985  0.315349  0.325500  0.324771  0.335500  0.294357   \n4     0.311000  0.278000  0.317500  0.287000  0.290500  0.325500  0.321000   \n...        ...       ...       ...       ...       ...       ...       ...   \n2032  0.183000  0.158000  0.165000  0.170500  0.155000  0.151000  0.145417   \n2033  0.150500  0.134500  0.168000  0.147500  0.166750  0.139000  0.137000   \n2034  0.283500  0.323000  0.277750  0.300500  0.263000  0.278500  0.277667   \n2035  0.363500  0.257500  0.320000  0.307500  0.288000  0.317000  0.256375   \n2036  0.134100  0.209920  0.224571  0.189226  0.217400  0.114920  0.178167   \n\n             8         9        10  VIP_SCORE  \n0     0.232500  0.262500  0.238000   0.245950  \n1     0.297000  0.325634  0.289396   0.319945  \n2     0.406000  0.415000  0.390500   0.391050  \n3     0.333759  0.315583  0.332383   0.321219  \n4     0.321000  0.319000  0.303500   0.307400  \n...        ...       ...       ...        ...  \n2032  0.196500  0.159500  0.197500   0.168142  \n2033  0.169500  0.126167  0.126000   0.146492  \n2034  0.229000  0.249500  0.250000   0.273242  \n2035  0.240000  0.318000  0.313000   0.298088  \n2036  0.234563  0.268000  0.208576   0.197944  \n\n[2037 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>VIP_SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.255500</td>\n      <td>0.246000</td>\n      <td>0.241500</td>\n      <td>0.224500</td>\n      <td>0.259500</td>\n      <td>0.234500</td>\n      <td>0.265000</td>\n      <td>0.232500</td>\n      <td>0.262500</td>\n      <td>0.238000</td>\n      <td>0.245950</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.291179</td>\n      <td>0.350667</td>\n      <td>0.333296</td>\n      <td>0.322452</td>\n      <td>0.319961</td>\n      <td>0.332547</td>\n      <td>0.337322</td>\n      <td>0.297000</td>\n      <td>0.325634</td>\n      <td>0.289396</td>\n      <td>0.319945</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.364500</td>\n      <td>0.377500</td>\n      <td>0.423000</td>\n      <td>0.358000</td>\n      <td>0.401500</td>\n      <td>0.366500</td>\n      <td>0.408000</td>\n      <td>0.406000</td>\n      <td>0.415000</td>\n      <td>0.390500</td>\n      <td>0.391050</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.293000</td>\n      <td>0.341985</td>\n      <td>0.315349</td>\n      <td>0.325500</td>\n      <td>0.324771</td>\n      <td>0.335500</td>\n      <td>0.294357</td>\n      <td>0.333759</td>\n      <td>0.315583</td>\n      <td>0.332383</td>\n      <td>0.321219</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.311000</td>\n      <td>0.278000</td>\n      <td>0.317500</td>\n      <td>0.287000</td>\n      <td>0.290500</td>\n      <td>0.325500</td>\n      <td>0.321000</td>\n      <td>0.321000</td>\n      <td>0.319000</td>\n      <td>0.303500</td>\n      <td>0.307400</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2032</th>\n      <td>0.183000</td>\n      <td>0.158000</td>\n      <td>0.165000</td>\n      <td>0.170500</td>\n      <td>0.155000</td>\n      <td>0.151000</td>\n      <td>0.145417</td>\n      <td>0.196500</td>\n      <td>0.159500</td>\n      <td>0.197500</td>\n      <td>0.168142</td>\n    </tr>\n    <tr>\n      <th>2033</th>\n      <td>0.150500</td>\n      <td>0.134500</td>\n      <td>0.168000</td>\n      <td>0.147500</td>\n      <td>0.166750</td>\n      <td>0.139000</td>\n      <td>0.137000</td>\n      <td>0.169500</td>\n      <td>0.126167</td>\n      <td>0.126000</td>\n      <td>0.146492</td>\n    </tr>\n    <tr>\n      <th>2034</th>\n      <td>0.283500</td>\n      <td>0.323000</td>\n      <td>0.277750</td>\n      <td>0.300500</td>\n      <td>0.263000</td>\n      <td>0.278500</td>\n      <td>0.277667</td>\n      <td>0.229000</td>\n      <td>0.249500</td>\n      <td>0.250000</td>\n      <td>0.273242</td>\n    </tr>\n    <tr>\n      <th>2035</th>\n      <td>0.363500</td>\n      <td>0.257500</td>\n      <td>0.320000</td>\n      <td>0.307500</td>\n      <td>0.288000</td>\n      <td>0.317000</td>\n      <td>0.256375</td>\n      <td>0.240000</td>\n      <td>0.318000</td>\n      <td>0.313000</td>\n      <td>0.298088</td>\n    </tr>\n    <tr>\n      <th>2036</th>\n      <td>0.134100</td>\n      <td>0.209920</td>\n      <td>0.224571</td>\n      <td>0.189226</td>\n      <td>0.217400</td>\n      <td>0.114920</td>\n      <td>0.178167</td>\n      <td>0.234563</td>\n      <td>0.268000</td>\n      <td>0.208576</td>\n      <td>0.197944</td>\n    </tr>\n  </tbody>\n</table>\n<p>2037 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df['VIP_SCORE'] = (model_df[1] + model_df[2] + model_df[3] + model_df[4] + \n",
    "    model_df[5] +model_df[6] + model_df[7] + model_df[8] + model_df[9] + model_df[10])/10\n",
    "\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge Results back to Test Data Frame\n",
    "test_df['VIP_SCORE'] = model_df['VIP_SCORE']\n",
    "test_df['ED_IP_VISIT'] = y_test_df\n",
    "test_df['MEMBER_ID'] = member_id_list\n",
    "test_df['DATA_DATE_START'] = data_date_start_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      MEMBER_ID  VIP_SCORE  ED_IP_VISIT DATA_DATE_START\n620  5365587178   0.774619            0      2022-11-01\n440  5365831518   0.754253            0      2022-11-01\n859  5365784285   0.749605            0      2022-11-01\n841  5364525796   0.749409            0      2022-11-01\n265  5365907435   0.723603            0      2022-11-01\n825  5365582644   0.715242            0      2022-11-01\n610  5365585495   0.701632            0      2022-11-01\n183  5365791675   0.675050            0      2022-11-01\n346  5365619083   0.669218            0      2022-11-01\n663  5365689546   0.659492            0      2022-11-01\n525  5365983627   0.651792            0      2022-11-01\n384  5365592331   0.641720            0      2022-11-01\n871  5365828190   0.634059            0      2022-11-01\n534  5365568845   0.634057            0      2022-11-01\n888  5365563028   0.627947            0      2022-11-01",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MEMBER_ID</th>\n      <th>VIP_SCORE</th>\n      <th>ED_IP_VISIT</th>\n      <th>DATA_DATE_START</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>620</th>\n      <td>5365587178</td>\n      <td>0.774619</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>5365831518</td>\n      <td>0.754253</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>859</th>\n      <td>5365784285</td>\n      <td>0.749605</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>841</th>\n      <td>5364525796</td>\n      <td>0.749409</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>265</th>\n      <td>5365907435</td>\n      <td>0.723603</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>825</th>\n      <td>5365582644</td>\n      <td>0.715242</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>5365585495</td>\n      <td>0.701632</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>5365791675</td>\n      <td>0.675050</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>346</th>\n      <td>5365619083</td>\n      <td>0.669218</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>663</th>\n      <td>5365689546</td>\n      <td>0.659492</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>525</th>\n      <td>5365983627</td>\n      <td>0.651792</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>5365592331</td>\n      <td>0.641720</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>5365828190</td>\n      <td>0.634059</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>534</th>\n      <td>5365568845</td>\n      <td>0.634057</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>5365563028</td>\n      <td>0.627947</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View Predicted Members DF\n",
    "predicted_df = test_df[['MEMBER_ID','VIP_SCORE','ED_IP_VISIT','DATA_DATE_START']]\n",
    "predicted_df = predicted_df.sort_values(by=['VIP_SCORE'],ascending = False)\n",
    "predicted_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "766ae136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      MEMBER_ID  VIP_SCORE  ED_IP_VISIT DATA_DATE_START\n620  5365587178   0.774619            0      2022-11-01\n440  5365831518   0.754253            0      2022-11-01\n859  5365784285   0.749605            0      2022-11-01\n841  5364525796   0.749409            0      2022-11-01\n265  5365907435   0.723603            0      2022-11-01\n825  5365582644   0.715242            0      2022-11-01\n610  5365585495   0.701632            0      2022-11-01\n183  5365791675   0.675050            0      2022-11-01\n346  5365619083   0.669218            0      2022-11-01\n663  5365689546   0.659492            0      2022-11-01\n525  5365983627   0.651792            0      2022-11-01\n384  5365592331   0.641720            0      2022-11-01\n871  5365828190   0.634059            0      2022-11-01\n534  5365568845   0.634057            0      2022-11-01\n888  5365563028   0.627947            0      2022-11-01",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MEMBER_ID</th>\n      <th>VIP_SCORE</th>\n      <th>ED_IP_VISIT</th>\n      <th>DATA_DATE_START</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>620</th>\n      <td>5365587178</td>\n      <td>0.774619</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>5365831518</td>\n      <td>0.754253</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>859</th>\n      <td>5365784285</td>\n      <td>0.749605</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>841</th>\n      <td>5364525796</td>\n      <td>0.749409</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>265</th>\n      <td>5365907435</td>\n      <td>0.723603</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>825</th>\n      <td>5365582644</td>\n      <td>0.715242</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>5365585495</td>\n      <td>0.701632</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>5365791675</td>\n      <td>0.675050</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>346</th>\n      <td>5365619083</td>\n      <td>0.669218</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>663</th>\n      <td>5365689546</td>\n      <td>0.659492</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>525</th>\n      <td>5365983627</td>\n      <td>0.651792</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>5365592331</td>\n      <td>0.641720</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>5365828190</td>\n      <td>0.634059</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>534</th>\n      <td>5365568845</td>\n      <td>0.634057</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>5365563028</td>\n      <td>0.627947</td>\n      <td>0</td>\n      <td>2022-11-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View Predicted Members DF\n",
    "predicted_df = test_df[['MEMBER_ID','VIP_SCORE','ED_IP_VISIT','DATA_DATE_START']]\n",
    "predicted_df = predicted_df.sort_values(by=['VIP_SCORE'],ascending = False)\n",
    "predicted_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # convert from object to datetime and format-not needed in CTL so check sql to see what diffs are\n",
    "# predicted_df['DATA_DATE_START'] = predicted_df['DATA_DATE_START'].astype('datetime64[ns]')\n",
    "# predicted_df.info()\n",
    "# predicted_df['DATA_DATE_START'] = predicted_df['DATA_DATE_START'].dt.strftime('%Y%m')\n",
    "# predicted_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df.to_csv('CCA Model - 202208.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d17587d",
   "metadata": {},
   "source": [
    "### Output and Update Tables in Snowflake for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e144d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Connection\n",
      "Database updated\n"
     ]
    }
   ],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "\n",
    "#Define parameters if neccessary\n",
    "warehouse = os.getenv('Snowflake_warehouse')\n",
    "database = os.getenv('Snowflake_database')\n",
    "schema = os.getenv('Snowflake_schema')\n",
    "\n",
    "#Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "#Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "try:\n",
    "    sql = 'USE DATABASE {}'.format(database)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    sql = 'USE SCHEMA {}.{}'.format(database,schema)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    #Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    #Query to Snowflake\n",
    "    sql = \"CREATE TABLE IF NOT EXISTS VIP_SCORING (CLNT string,MEMBER_ID string, VIP_SCORE float ,DATA_DATE_START string)\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    entry_list = []\n",
    "    for row in predicted_df.iterrows():\n",
    "        client = 'CCA'\n",
    "        member_id = str(row[1][0])\n",
    "        vip_score = row[1][1]\n",
    "        data_date = str(row[1][3])\n",
    "        entry = (client,member_id,vip_score,data_date)\n",
    "        entry_list.append(entry)       \n",
    "    entry = str(entry_list)[1:len(str(entry_list))-1]\n",
    "    sql = 'INSERT INTO VIP_SCORING (CLNT,MEMBER_ID, VIP_SCORE, DATA_DATE_START) VALUES {}'.format(entry)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)  \n",
    "    cursor.close\n",
    "\n",
    "    print('Database updated')\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88077bed",
   "metadata": {},
   "source": [
    "# ITEMS BELOW THIS ARE FOR ANALYSIS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1182c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction List\n",
    "for row in predicted_df.iterrows():\n",
    "    print('Member: ',row[1][0],' VIP_SCORE: ',round(row[1][1],4), ' ED_IP_VISIT: ',row[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc46052",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_threshold = 0.3\n",
    "\n",
    "final_y_pred = [1 if result >= final_threshold else 0 for result in test_df['VIP_SCORE'].tolist()]\n",
    "\n",
    "# Creating confusion maxtrix\n",
    "new_cnf_matrix = metrics.confusion_matrix(y_test_df,final_y_pred)\n",
    "\n",
    "%matplotlib inline\n",
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(new_cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# Axis labels\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#Calulating Metrics\n",
    "accuracy = metrics.accuracy_score(y_test_df, final_y_pred)\n",
    "precision = metrics.precision_score(y_test_df, final_y_pred)\n",
    "recall = metrics.recall_score(y_test_df, final_y_pred)\n",
    "    \n",
    "print('Thereshold: ',final_threshold)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realtime Accuracy\n",
    "X = []\n",
    "Y = []\n",
    "count = 1\n",
    "sums = 0\n",
    "for row in predicted_df.iterrows():\n",
    "    sums += row[1][2]\n",
    "    accuracy = round(sums/count *100,2)\n",
    "    X.append(count)\n",
    "    Y.append(accuracy)\n",
    "    print('After',count, 'prediction, the realtime accuarcy is ',accuracy)\n",
    "    count +=1\n",
    "    \n",
    "plt.plot(X, Y, marker='.', label='Random Forest')\n",
    "\n",
    "acc = np.where(np.array(Y) <= 60)\n",
    "\n",
    "acc = np.where(np.array(Y) <= 60)\n",
    "#for i,x in enumerate(acc[0]):\n",
    "    #print(i,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71bfc5",
   "metadata": {},
   "source": [
    "### Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3856ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [x for x in df.columns if x != 'ED_IP_VISIT']\n",
    "\n",
    "importance_df = pd.DataFrame()\n",
    "\n",
    "for i,clf in forest_dict.items():\n",
    "    start_time = time.time()\n",
    "    importances = list(clf.feature_importances_)\n",
    "    feature_importances = [(feature, round(importance, 15)) for feature, importance in zip(feature_list, importances)]\n",
    "    importance_df[i+1] = feature_importances\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "importance_avg_list = []\n",
    "\n",
    "for row in importance_df.iterrows():\n",
    "    feature_name = row[1][1][0]\n",
    "    avg = round((row[1][1][1] + row[1][2][1] + row[1][3][1] + row[1][4][1] + row[1][5][1] + row[1][6][1] + row[1][7][1] \n",
    "            + row[1][8][1] + row[1][9][1] + row[1][10][1])/10,4)\n",
    "    \n",
    "    importance_avg_list.append((feature_name,avg))\n",
    "    \n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(importance_avg_list, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "feature_importances[0:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb55d49",
   "metadata": {},
   "source": [
    "### Random Search with Cross Validation - Hyperparameter Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(random_state = 22822)\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scoring = {\"Accuracy\":make_scorer(metrics.accuracy_score),\"Precision\":make_scorer(metrics.precision_score),\n",
    "            \"Recall\":make_scorer(metrics.recall_score),\"AUC\":make_scorer(metrics.roc_auc_score)}\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='balanced_accuracy', \n",
    "                              cv = 3, verbose=6, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train,y_train);\n",
    "\n",
    "#Best parameters found\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f54ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.cv_results_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NewBase')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bb5f7e03c56d6e91378b915f266587dc28bbd5e1a358e8c73c01ed5dd6d43c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
