{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f43ec2",
   "metadata": {},
   "source": [
    "# ED-IP Prediction with Random Forests\n",
    "\n",
    "The goal of the model is to predict whether or not a member is going to have an ED or IP visit in the next 180 following the latest claim stratification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23059ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary dependencies\n",
    "import pandas as pd\n",
    "import snowflake.connector as sf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31578cb0",
   "metadata": {},
   "source": [
    "### Extracting Data Frame from Snowflake Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Connection\n",
      "Successful DataFrame Created\n",
      "Ready for Cleaning\n"
     ]
    }
   ],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "\n",
    "# Define warehouse, if neccessary\n",
    "warehouse = 'DEVELOPER_BASIC'\n",
    "\n",
    "# Define Database, if not defined in SQL request\n",
    "#database = 'VESTA_STAGING'\n",
    "\n",
    "# Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "# Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "\n",
    "try:\n",
    "    # If defining a database, uncomment code set and add database in connection parameter\n",
    "    #sql = 'use {}'.format(database)\n",
    "    #execute_query(conn,sql)\n",
    "    \n",
    "    # Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    # Query to Snowflake\n",
    "    sql = '''WITH EDIP AS ( //This is sub table for a self join\n",
    "\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"CLAIMS_REPORTING\".\"UHC_NE_MEM_PROFILE_IP_ER_SNF\" //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    WHERE MEASURE = 'ED' or MEASURE = 'IP' \n",
    "\n",
    "    ),\n",
    "\n",
    "    EDIPTABLE AS ( //This table shows the Member ID, date start, and the number of ED/IP in the next 6 months\n",
    "\n",
    "    SELECT\n",
    "        SCORE.MEMBER_ID,\n",
    "        TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "        COUNT(DISTINCT EDIP.DOS_FROM) AS ED_IP_VISITS_IN_NEXT_6_MONTHS\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "        LEFT JOIN EDIP \n",
    "            ON SCORE.MEMBER_ID = EDIP.MEMBER_ID\n",
    "                AND EDIP.DOS_FROM > TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))\n",
    "                AND DATEDIFF(days,TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) >= 45\n",
    "                AND DATEDIFF(days, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) <= 180\n",
    "    GROUP BY SCORE.MEMBER_ID,\n",
    "        DATE_START \n",
    "\n",
    "    )\n",
    "\n",
    "SELECT\n",
    "    //TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "    //DATA_DATE_START,\n",
    "    //SCORE.MEMBER_ID,\n",
    "    CASE WHEN EDIPTABLE.ED_IP_VISITS_IN_NEXT_6_MONTHS > 0 THEN 1 ELSE 0 END as ED_IP_VISIT,\n",
    "    ACSC__COUNT,\n",
    "    ACSC__SCORE,\n",
    "    ACSC_A_FIB_AND_FLUTTER,\n",
    "    ACSC_ALCOHOL_RELATED,\n",
    "    ACSC_ANEMIA,\n",
    "    ACSC_ANGINA,\n",
    "    ACSC_ASTHMA,\n",
    "    ACSC_CELLULITIS,\n",
    "    ACSC_CONGESTIVE_HEART_FAILURE,\n",
    "    ACSC_CONSTIPATION,\n",
    "    ACSC_CONVULSION_EPILEPSY,\n",
    "    ACSC_COPD,\n",
    "    ACSC_DECUBITI_STAGE_3_,\n",
    "    ACSC_DEHYDRATION_GASTROENTERITIS,\n",
    "    ACSC_DIABETES_COMPLICATIONS,\n",
    "    ACSC_DYSPEPSIA,\n",
    "    ACSC_ENT_INFECTION,\n",
    "    ACSC_HYPERTENSION,\n",
    "    ACSC_HYPOGLYCEMIA,\n",
    "    ACSC_HYPOKALEMIA,\n",
    "    ACSC_INFLUENZA_PNEUMONIA,\n",
    "    ACSC_MIGRAINE_HEADACHE,\n",
    "    ACSC_NUTRITION_DEFICIENT,\n",
    "    ACSC_PERFORATED_BLEEDING_ULCER,\n",
    "    ACSC_PROXIMAL_FEMUR_FRACTURE,\n",
    "    ACSC_PYELONEPHRITIS,\n",
    "    ACSC_UTI,\n",
    "    ACSC_VACCINE_PREVENTABLE_DX,\n",
    "    DATEDIFF(year,DOB, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))) as AGE,\n",
    "    AMB_ACSC_COST,\n",
    "    AMB_ACSC_COUNT,\n",
    "    BH__COUNT,\n",
    "    BH__SCORE,\n",
    "    BH_ALTERED_MENTAL_STATE,\n",
    "    BH_ALZHEIMERS_DEMENTIA,\n",
    "    BH_ANXIETY,\n",
    "    BH_BI_POLAR,\n",
    "    BH_DEPRESSION,\n",
    "    BH_SCHIZOPHRENIA,\n",
    "    BH_SUBABUSE,\n",
    "    CRN__COUNT,\n",
    "    CRN_AFIB,\n",
    "    CRN_ASTHMA,\n",
    "    CRN_CARDIOVASCULAR_DX,\n",
    "    CRN_CHRONIC_KIDNEY_DISEASE,\n",
    "    CRN_CONGESTIVE_HEART_FAILURE,\n",
    "    CRN_COPD,\n",
    "    CRN_DIABETES_W__ACUTE_COMP,\n",
    "    CRN_DIABETES_W__CHRONIC_COMP,\n",
    "    CRN_DIABETES_W_OUT_COMP,\n",
    "    CRN_FALLS,\n",
    "    CRN_GASTRO_ESOPH_REFLUX,\n",
    "    CRN_HIP_FRACTURE,\n",
    "    CRN_HTN,\n",
    "    CRN_OBESITY,\n",
    "    CRN_OSTEOPOROSIS,\n",
    "    CRN_PARKINSONS_DISEASE,\n",
    "    CRN_PRESSURE_ULCER,\n",
    "    CRN_PRIOR_MI,\n",
    "    CRN_PRIOR_STROKE,\n",
    "    CRN_SCORE,\n",
    "    CRN_SLEEP_APNEA,\n",
    "    CRN_SMOKING,\n",
    "    CRN_UTI,\n",
    "    DYAD_CKD_DD,\n",
    "    DYAD_CKD_OP,\n",
    "    DYAD_COPD_DD,\n",
    "    DYAD_COPD_HF,\n",
    "    DYAD_COPD_OP,\n",
    "    DYAD_COUNT,\n",
    "    DYAD_DM_CKD,\n",
    "    DYAD_DM_OP,\n",
    "    DYAD_HBP_HF,\n",
    "    DYAD_HF_CKD,\n",
    "    ED_ACSC_COST,\n",
    "    ED_ACSC_COUNT,\n",
    "    GENDER,\n",
    "    CASE WHEN \"GROUP\" = 'E' THEN 'A' ELSE \"GROUP\" END as \"GROUP\",\n",
    "    HMKR_ACSC_COST,\n",
    "    HMKR_ACSC_COUNT,\n",
    "    HTI_RISK_SCORE_V2_1,\n",
    "    IP_ACSC_COST,\n",
    "    IP_ACSC_COUNT,\n",
    "    IP_READMIT_ACSC_COST,\n",
    "    IP_READMIT_ACSC_COUNT,\n",
    "    IP_RHB_ACSC_COST,\n",
    "    IP_RHB_ACSC_COUNT,\n",
    "    CASE WHEN LANGUAGE_SPOKEN is NULL THEN 'Unknown'\n",
    "         WHEN LANGUAGE_SPOKEN = 'English' THEN 'English'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Chinese' THEN 'Chinese'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Spanish' THEN 'Spanish'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Russian' THEN 'Russian'\n",
    "    ELSE 'Other' END AS LANGUAGE_SPOKEN_CLEAN,\n",
    "    NI_COST_DENT,\n",
    "    NI_COST_ED,\n",
    "    NI_COST_HM,\n",
    "    NI_COST_HMKR,\n",
    "    NI_COST_HS,\n",
    "    NI_COST_IP,\n",
    "    NI_COST_IP_RHB,\n",
    "    NI_COST_OP,\n",
    "    NI_COST_OTH,\n",
    "    NI_COST_PCA_T1020,\n",
    "    NI_COST_PCA_T1019,\n",
    "    NI_COST_PR,\n",
    "    NI_COST_PSYC,\n",
    "    NI_COST_RX,\n",
    "    NI_COUNT_DENT,\n",
    "    NI_COUNT_ED,\n",
    "    NI_COUNT_HM,\n",
    "    NI_COUNT_HMKR,\n",
    "    NI_COUNT_HS,\n",
    "    NI_COUNT_IP,\n",
    "    NI_COUNT_IP_RHB,\n",
    "    NI_COUNT_OP,\n",
    "    NI_COUNT_OTH,\n",
    "    NI_COUNT_PCA_T1020,\n",
    "    NI_COUNT_PCA_T1019,\n",
    "    NI_COUNT_PR,\n",
    "    NI_COUNT_PSYC,\n",
    "    NI_COUNT_RX,\n",
    "    NON_IMPACTABLE_CLAIM_COUNT,\n",
    "    OP_ACSC_COST,\n",
    "    OP_ACSC_COUNT,\n",
    "    CAST(PART_C_RISK_SCORE as FLOAT) as PART_C_RISK_SCORE,\n",
    "    PCA_T1020_ACSC_COUNT,\n",
    "    PCA_T1020_ACSC_COST,\n",
    "    PCA_T1019_ACSC_COUNT,\n",
    "    PCA_T1019_ACSC_COST,\n",
    "    PR_ACSC_COST,\n",
    "    PR_ACSC_COUNT,\n",
    "    CASE WHEN RC is NULL THEN 'UNDEFINED' ELSE RC END AS RC_CLEAN,\n",
    "    SNF_COST,\n",
    "    SNF_COUNT,\n",
    "    TOTAL_IMPACTABLE_COST,\n",
    "    TOTAL_IMPACTABLE_COST_PRO,\n",
    "    TOTAL_NON_IMPACTABLE_COST\n",
    "FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "    LEFT JOIN EDIPTABLE\n",
    "        ON SCORE.MEMBER_ID = EDIPTABLE.MEMBER_ID\n",
    "            AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) = EDIPTABLE.DATE_START\n",
    "WHERE SCORE.CLNT = 'UHC_NE' //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) \n",
    "    < TO_DATE(CONCAT(LEFT(CURRENT_DATE-210,7),'-01')) //This is looking at files that have had a reasonable amount of time to process'''\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # Dataframe creation\n",
    "    df = pd.DataFrame.from_records(iter(cursor), columns = [x[0] for x in cursor.description])\n",
    "    \n",
    "    print('Successful DataFrame Created')\n",
    "    \n",
    "    cursor.close\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close\n",
    "    \n",
    "print('Ready for Cleaning')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning Operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Find features with missing values\n",
    "sumdf = pd.DataFrame(df.isna().sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features that initially had missing values within the data frame: \n",
      " ['ACSC_A_FIB_AND_FLUTTER', 'ACSC_ALCOHOL_RELATED', 'ACSC_ANEMIA', 'ACSC_ANGINA', 'ACSC_ASTHMA', 'ACSC_CELLULITIS', 'ACSC_CONSTIPATION', 'ACSC_COPD', 'ACSC_DECUBITI_STAGE_3_', 'ACSC_DEHYDRATION_GASTROENTERITIS', 'ACSC_ENT_INFECTION', 'ACSC_HYPOGLYCEMIA', 'ACSC_HYPOKALEMIA', 'ACSC_MIGRAINE_HEADACHE', 'ACSC_PERFORATED_BLEEDING_ULCER', 'ACSC_PROXIMAL_FEMUR_FRACTURE', 'ACSC_PYELONEPHRITIS', 'ACSC_VACCINE_PREVENTABLE_DX', 'BH_SUBABUSE', 'CRN_PRIOR_MI', 'CRN_SMOKING', 'NI_COST_DENT', 'NI_COST_HMKR', 'NI_COST_IP_RHB', 'NI_COST_PCA_T1020', 'NI_COST_PCA_T1019', 'NI_COUNT_DENT', 'NI_COUNT_HMKR', 'NI_COUNT_IP_RHB', 'NI_COUNT_PCA_T1020', 'NI_COUNT_PCA_T1019'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "# Review features with missing values\n",
    "print('These are the features that initially had missing values within the data frame: \\n',features_to_drop,'\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Add nulls to correct printout from above to this list\n",
    "Nulls_to_correct = ['ACSC_A_FIB_AND_FLUTTER', 'ACSC_ALCOHOL_RELATED', 'ACSC_ANEMIA', 'ACSC_ANGINA', 'ACSC_ASTHMA', 'ACSC_CELLULITIS', 'ACSC_CONSTIPATION', 'ACSC_COPD', 'ACSC_DECUBITI_STAGE_3_', 'ACSC_DEHYDRATION_GASTROENTERITIS', 'ACSC_ENT_INFECTION', 'ACSC_HYPOGLYCEMIA', 'ACSC_HYPOKALEMIA', 'ACSC_MIGRAINE_HEADACHE', 'ACSC_PERFORATED_BLEEDING_ULCER', 'ACSC_PROXIMAL_FEMUR_FRACTURE', 'ACSC_PYELONEPHRITIS', 'ACSC_VACCINE_PREVENTABLE_DX', 'BH_SUBABUSE', 'CRN_PRIOR_MI', 'CRN_SMOKING', 'NI_COST_DENT', 'NI_COST_HMKR', 'NI_COST_IP_RHB', 'NI_COST_PCA_T1020', 'NI_COST_PCA_T1019', 'NI_COUNT_DENT', 'NI_COUNT_HMKR', 'NI_COUNT_IP_RHB', 'NI_COUNT_PCA_T1020', 'NI_COUNT_PCA_T1019']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These columns were dropped after because missing values were not corrected :\n",
      " [] \n",
      "\n",
      "\n",
      "These are the features that were converted to dummy variables: \n",
      " ['GENDER', 'GROUP', 'LANGUAGE_SPOKEN_CLEAN', 'RC_CLEAN'] \n",
      "\n",
      "\n",
      "ED_IP_VISIT                        int64\n",
      "ACSC__COUNT                      float64\n",
      "ACSC__SCORE                      float64\n",
      "ACSC_A_FIB_AND_FLUTTER           float64\n",
      "ACSC_ALCOHOL_RELATED             float64\n",
      "                                  ...   \n",
      "TOTAL_NON_IMPACTABLE_COST        float64\n",
      "GENDER_MALE                        uint8\n",
      "GROUP_C                            uint8\n",
      "GROUP_D                            uint8\n",
      "LANGUAGE_SPOKEN_CLEAN_Unknown      uint8\n",
      "Length: 133, dtype: object\n",
      "Memory Usage of Dataframe: 37102396 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": "   ED_IP_VISIT  ACSC__COUNT  ACSC__SCORE  ACSC_A_FIB_AND_FLUTTER  \\\n0            0          0.0          0.0                     0.0   \n1            0          0.0          0.0                     0.0   \n2            0          0.0          0.0                     0.0   \n3            0          0.0          0.0                     0.0   \n4            0          0.0          0.0                     0.0   \n\n   ACSC_ALCOHOL_RELATED  ACSC_ANEMIA  ACSC_ANGINA  ACSC_ASTHMA  \\\n0                   0.0          0.0          0.0          0.0   \n1                   0.0          0.0          0.0          0.0   \n2                   0.0          0.0          0.0          0.0   \n3                   0.0          0.0          0.0          0.0   \n4                   0.0          0.0          0.0          0.0   \n\n   ACSC_CELLULITIS  ACSC_CONGESTIVE_HEART_FAILURE  ...  PR_ACSC_COUNT  \\\n0                0                            0.0  ...            0.0   \n1                0                            0.0  ...            0.0   \n2                0                            0.0  ...            0.0   \n3                0                            0.0  ...            0.0   \n4                0                            0.0  ...            0.0   \n\n   SNF_COST  SNF_COUNT  TOTAL_IMPACTABLE_COST  TOTAL_IMPACTABLE_COST_PRO  \\\n0       0.0        0.0                    0.0                        0.0   \n1       0.0        0.0                    0.0                        0.0   \n2       0.0        0.0                    0.0                        0.0   \n3       0.0        0.0                    0.0                        0.0   \n4       0.0        0.0                    0.0                        0.0   \n\n   TOTAL_NON_IMPACTABLE_COST  GENDER_MALE  GROUP_C  GROUP_D  \\\n0                    1815.32            1        0        1   \n1                    2409.08            1        0        1   \n2                       5.00            0        0        1   \n3                    1430.67            1        0        1   \n4                     163.50            0        0        1   \n\n   LANGUAGE_SPOKEN_CLEAN_Unknown  \n0                              0  \n1                              1  \n2                              0  \n3                              1  \n4                              1  \n\n[5 rows x 133 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ED_IP_VISIT</th>\n      <th>ACSC__COUNT</th>\n      <th>ACSC__SCORE</th>\n      <th>ACSC_A_FIB_AND_FLUTTER</th>\n      <th>ACSC_ALCOHOL_RELATED</th>\n      <th>ACSC_ANEMIA</th>\n      <th>ACSC_ANGINA</th>\n      <th>ACSC_ASTHMA</th>\n      <th>ACSC_CELLULITIS</th>\n      <th>ACSC_CONGESTIVE_HEART_FAILURE</th>\n      <th>...</th>\n      <th>PR_ACSC_COUNT</th>\n      <th>SNF_COST</th>\n      <th>SNF_COUNT</th>\n      <th>TOTAL_IMPACTABLE_COST</th>\n      <th>TOTAL_IMPACTABLE_COST_PRO</th>\n      <th>TOTAL_NON_IMPACTABLE_COST</th>\n      <th>GENDER_MALE</th>\n      <th>GROUP_C</th>\n      <th>GROUP_D</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Unknown</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1815.32</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2409.08</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1430.67</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>163.50</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 133 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill selected features with 0 value\n",
    "for col in Nulls_to_correct:\n",
    "    df[col] = df[col].fillna(0)\n",
    "# Check for missing values and drop columns with missing values\n",
    "sumdf = pd.DataFrame(df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "print('These columns were dropped after because missing values were not corrected :\\n', features_to_drop,'\\n\\n')\n",
    "df = df.drop(columns = features_to_drop)\n",
    "\n",
    "# Convert object datatypes to dummy variables\n",
    "object_list = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes == 'object':\n",
    "        object_list.append(col)\n",
    "\n",
    "\n",
    "print('These are the features that were converted to dummy variables: \\n',object_list,'\\n\\n')\n",
    "df = pd.get_dummies(df, columns = object_list, drop_first = True)\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "original_memory = df.memory_usage().sum()\n",
    "print(f'Memory Usage of Dataframe: {df.memory_usage().sum()} bytes')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Converting float 64 data types to float 32 to reduce memory usage on local machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage of Original Dataframe: 37102396 bytes\n",
      "Memory Usage of New Dataframe: 20055408 bytes\n",
      "Memory usage reduced by:46.0%\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35813 entries, 0 to 35812\n",
      "Columns: 133 entries, ED_IP_VISIT to LANGUAGE_SPOKEN_CLEAN_Unknown\n",
      "dtypes: float32(119), int64(10), uint8(4)\n",
      "memory usage: 19.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "   ED_IP_VISIT  ACSC__COUNT  ACSC__SCORE  ACSC_A_FIB_AND_FLUTTER  \\\n0            0          0.0          0.0                     0.0   \n1            0          0.0          0.0                     0.0   \n2            0          0.0          0.0                     0.0   \n3            0          0.0          0.0                     0.0   \n4            0          0.0          0.0                     0.0   \n\n   ACSC_ALCOHOL_RELATED  ACSC_ANEMIA  ACSC_ANGINA  ACSC_ASTHMA  \\\n0                   0.0          0.0          0.0          0.0   \n1                   0.0          0.0          0.0          0.0   \n2                   0.0          0.0          0.0          0.0   \n3                   0.0          0.0          0.0          0.0   \n4                   0.0          0.0          0.0          0.0   \n\n   ACSC_CELLULITIS  ACSC_CONGESTIVE_HEART_FAILURE  ...  PR_ACSC_COUNT  \\\n0                0                            0.0  ...            0.0   \n1                0                            0.0  ...            0.0   \n2                0                            0.0  ...            0.0   \n3                0                            0.0  ...            0.0   \n4                0                            0.0  ...            0.0   \n\n   SNF_COST  SNF_COUNT  TOTAL_IMPACTABLE_COST  TOTAL_IMPACTABLE_COST_PRO  \\\n0       0.0        0.0                    0.0                        0.0   \n1       0.0        0.0                    0.0                        0.0   \n2       0.0        0.0                    0.0                        0.0   \n3       0.0        0.0                    0.0                        0.0   \n4       0.0        0.0                    0.0                        0.0   \n\n   TOTAL_NON_IMPACTABLE_COST  GENDER_MALE  GROUP_C  GROUP_D  \\\n0                1815.319946            1        0        1   \n1                2409.080078            1        0        1   \n2                   5.000000            0        0        1   \n3                1430.670044            1        0        1   \n4                 163.500000            0        0        1   \n\n   LANGUAGE_SPOKEN_CLEAN_Unknown  \n0                              0  \n1                              1  \n2                              0  \n3                              1  \n4                              1  \n\n[5 rows x 133 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ED_IP_VISIT</th>\n      <th>ACSC__COUNT</th>\n      <th>ACSC__SCORE</th>\n      <th>ACSC_A_FIB_AND_FLUTTER</th>\n      <th>ACSC_ALCOHOL_RELATED</th>\n      <th>ACSC_ANEMIA</th>\n      <th>ACSC_ANGINA</th>\n      <th>ACSC_ASTHMA</th>\n      <th>ACSC_CELLULITIS</th>\n      <th>ACSC_CONGESTIVE_HEART_FAILURE</th>\n      <th>...</th>\n      <th>PR_ACSC_COUNT</th>\n      <th>SNF_COST</th>\n      <th>SNF_COUNT</th>\n      <th>TOTAL_IMPACTABLE_COST</th>\n      <th>TOTAL_IMPACTABLE_COST_PRO</th>\n      <th>TOTAL_NON_IMPACTABLE_COST</th>\n      <th>GENDER_MALE</th>\n      <th>GROUP_C</th>\n      <th>GROUP_D</th>\n      <th>LANGUAGE_SPOKEN_CLEAN_Unknown</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1815.319946</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2409.080078</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1430.670044</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>163.500000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 133 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert data types\n",
    "#changing all float64 to float32\n",
    "df[df.select_dtypes(np.float64).columns] = df.select_dtypes(np.float64).astype(np.float32)\n",
    "new_memory = df.memory_usage().sum()\n",
    "print(f'Memory Usage of Original Dataframe: {original_memory} bytes')\n",
    "print(f'Memory Usage of New Dataframe: {new_memory} bytes')\n",
    "print(f'Memory usage reduced by:{round((original_memory-new_memory)/original_memory * 100,0)}%')\n",
    "df.info()\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option to remove low variability features\n",
    "\n",
    "This block is here to run as optional pre-processing. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features that were dropped because of low variability: \n",
      " ['ACSC_CELLULITIS', 'ACSC_VACCINE_PREVENTABLE_DX', 'HMKR_ACSC_COST', 'HMKR_ACSC_COUNT', 'NI_COST_DENT', 'NI_COST_HMKR', 'NI_COST_PCA_T1020', 'NI_COST_PCA_T1019', 'NI_COUNT_DENT', 'NI_COUNT_HMKR', 'NI_COUNT_PCA_T1020', 'NI_COUNT_PCA_T1019', 'PART_C_RISK_SCORE', 'PCA_T1020_ACSC_COUNT', 'PCA_T1020_ACSC_COST', 'PCA_T1019_ACSC_COUNT', 'PCA_T1019_ACSC_COST'] \n",
      "\n",
      "\n",
      "Memory Usage of Dataframe: 16474108 bytes\n"
     ]
    }
   ],
   "source": [
    "# Drop low variability columns\n",
    "df_var = df.var()\n",
    "df.columns.to_list()\n",
    "\n",
    "features_to_drop = []\n",
    "\n",
    "for i in range(len(df.columns.to_list())):\n",
    "    #print(df.columns.to_list()[i],df_var[i])\n",
    "    if df_var[i] == 0 and df.columns.to_list()[i] != 'ED_IP_VISIT':\n",
    "        features_to_drop.append(df.columns.to_list()[i])\n",
    "\n",
    "        \n",
    "print('These are the features that were dropped because of low variability: \\n',features_to_drop,'\\n\\n')\n",
    "        \n",
    "df = df.drop(columns = features_to_drop)\n",
    "print(f'Memory Usage of Dataframe: {df.memory_usage().sum()} bytes')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35813 entries, 0 to 35812\n",
      "Columns: 116 entries, ED_IP_VISIT to LANGUAGE_SPOKEN_CLEAN_Unknown\n",
      "dtypes: float32(110), int64(2), uint8(4)\n",
      "memory usage: 15.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('UHC_NE_Model.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spliting, Scaling, and SMOTE (Synthetic Minority Oversampling Technique)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data set\n",
    "X = df[[col for col in df.columns if col != 'ED_IP_VISIT']] #independent variables\n",
    "y = df[[col for col in df.columns if col == 'ED_IP_VISIT']] #dependent variable\n",
    "y = y.values.flatten()\n",
    "\n",
    "# Define MinMax Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Transform data\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30, random_state = 2)\n",
    "\n",
    "# Smote for balancing the training data set\n",
    "smote = SMOTE(random_state = 2)\n",
    "X_train,y_train = smote.fit_resample(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating and Training Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a Random Forest Classifier\n",
    "clf=RandomForestClassifier(n_estimators = 2000,min_samples_split = 2, min_samples_leaf = 1,\n",
    "                           max_depth = 50, bootstrap = False, n_jobs = -1,random_state = 2)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train,y_train)\n",
    "print(f'Memory Usage of Dataframe: {df.memory_usage().sum()} bytes')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reporting on Performace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create probabilities from the model on test data\n",
    "y_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Store probabilites in Datafram for threshold analysis\n",
    "threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "for threshold in threshold_list:\n",
    "    y_pred = [1 if result >= threshold else 0 for result in y_prob]\n",
    "   \n",
    "    #Calulating Metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    \n",
    "    #print('Thereshold: ',threshold)\n",
    "    #print(\"Accuracy: \",accuracy)\n",
    "    #print(\"Precision: \",precision)\n",
    "    #print(\"Recall: \",recall)\n",
    "    \n",
    "metric_df = pd.DataFrame()\n",
    "metric_df['Threshold'] = threshold_list\n",
    "metric_df['Accuracy'] = accuracy_list\n",
    "metric_df['Precision'] = precision_list\n",
    "metric_df['Recall'] = recall_list\n",
    "metric_df['F1'] = (2 * metric_df['Precision'] * metric_df['Recall']) / (metric_df['Precision'] + metric_df['Recall'])\n",
    "metric_df['Acc + Recall'] = metric_df['Accuracy'] + metric_df['Recall']\n",
    "\n",
    "metric_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix for Threshold with Max Accuracy + Recall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Find the max accuracy and recall from the Metric Table and corresponding Threshold\n",
    "threshold = metric_df[metric_df['Acc + Recall'] == metric_df['Acc + Recall'].max()]['Threshold'].item()\n",
    "\n",
    "y_pred = [1 if result >= threshold else 0 for result in y_prob]\n",
    "\n",
    "\n",
    "# Creating confusion maxtrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "%matplotlib inline\n",
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# Axis labels\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Calulating Metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "print('Thereshold: ',threshold)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print('F1: ', (2*precision*recall)/(precision+recall))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ROC and AUC and Precision-Recall Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,  y_prob)\n",
    "\n",
    "# Print(thresholds)\n",
    "auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(loc=4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Calculate precision and recall for each threshold\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# Calculate scores\n",
    "f1, auc = metrics.f1_score(y_test, y_pred), metrics.auc(recall, precision)\n",
    "\n",
    "# Plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='Random Forest')\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ensemble of Random Forests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 29.92331099510193 seconds ---\n",
      "--- 39.13900971412659 seconds ---\n",
      "--- 49.83404755592346 seconds ---\n",
      "--- 59.05246376991272 seconds ---\n",
      "--- 69.4903244972229 seconds ---\n",
      "--- 81.52354526519775 seconds ---\n",
      "--- 89.18829274177551 seconds ---\n",
      "--- 100.28168296813965 seconds ---\n",
      "--- 161.4642903804779 seconds ---\n",
      "--- 146.49493288993835 seconds ---\n"
     ]
    }
   ],
   "source": [
    "forest_dict = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Split the data set\n",
    "    X = df[[col for col in df.columns if col != 'ED_IP_VISIT']] #independent variables\n",
    "    y = df[[col for col in df.columns if col == 'ED_IP_VISIT']] #dependent variable\n",
    "    y = y.values.flatten()\n",
    "\n",
    "    # Define MinMax Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Transform data\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split X and y into training and testing sets\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30, random_state = i)\n",
    "\n",
    "    # Smote for balancing the training data set\n",
    "    smote = SMOTE(random_state = i)\n",
    "    X_train,y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Create a Random Forest Classifier\n",
    "    clf=RandomForestClassifier(n_estimators = 2000,min_samples_split = 2, min_samples_leaf = 1,\n",
    "                               max_depth = 50, bootstrap = False, n_jobs = -1,random_state = i)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    #Store Random Forest\n",
    "    forest_dict[i] = clf\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New Query for Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Connection\n",
      "Successful DataFrame Created\n",
      "Ready for Cleaning\n",
      "(2057, 118)\n"
     ]
    }
   ],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "\n",
    "# Define warehouse, if neccessary\n",
    "warehouse = 'DEVELOPER_BASIC'\n",
    "\n",
    "# Define Database, if not defined in SQL request\n",
    "#database = 'VESTA_STAGING'\n",
    "\n",
    "# Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "# Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "try:\n",
    "    # If defining a database, uncomment code set and add database in connection parameter\n",
    "    #sql = 'use {}'.format(database)\n",
    "    #execute_query(conn,sql)\n",
    "    \n",
    "    # Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    # Query to Snowflake\n",
    "    sql = '''\n",
    "WITH EDIP AS ( //This is sub table for a self join\n",
    "\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"CLAIMS_REPORTING\".\"UHC_NE_MEM_PROFILE_IP_ER_SNF\" //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    WHERE MEASURE = 'ED' or MEASURE = 'IP' \n",
    "\n",
    "    ),\n",
    "\n",
    "EDIPTABLE AS ( //This table shows the Member ID, date start, and the number of ED/IP in the next 6 months\n",
    "\n",
    "    SELECT\n",
    "        SCORE.MEMBER_ID,\n",
    "        TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "        COUNT(DISTINCT EDIP.DOS_FROM) AS ED_IP_VISITS_IN_NEXT_6_MONTHS\n",
    "    FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "        LEFT JOIN EDIP \n",
    "            ON SCORE.MEMBER_ID = EDIP.MEMBER_ID\n",
    "                AND EDIP.DOS_FROM > TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))\n",
    "                AND DATEDIFF(days,TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) >= 45\n",
    "                AND DATEDIFF(days, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')),EDIP.DOS_FROM) <= 180\n",
    "    GROUP BY SCORE.MEMBER_ID,\n",
    "        DATE_START \n",
    "\n",
    "    )\n",
    "--commented out columns are because of features that were dropped due to low variability in original sql query\n",
    "SELECT\n",
    "    //TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) AS DATE_START,\n",
    "    DATA_DATE_START,\n",
    "    SCORE.MEMBER_ID,\n",
    "    CASE WHEN EDIPTABLE.ED_IP_VISITS_IN_NEXT_6_MONTHS > 0 THEN 1 ELSE 0 END as ED_IP_VISIT,\n",
    "    ACSC__COUNT,\n",
    "    ACSC__SCORE,\n",
    "    ACSC_A_FIB_AND_FLUTTER,\n",
    "    ACSC_ALCOHOL_RELATED,\n",
    "    ACSC_ANEMIA,\n",
    "    ACSC_ANGINA,\n",
    "    ACSC_ASTHMA,\n",
    "    --ACSC_CELLULITIS,\n",
    "    ACSC_CONGESTIVE_HEART_FAILURE,\n",
    "    ACSC_CONSTIPATION,\n",
    "    ACSC_CONVULSION_EPILEPSY,\n",
    "    ACSC_COPD,\n",
    "    ACSC_DECUBITI_STAGE_3_,\n",
    "    ACSC_DEHYDRATION_GASTROENTERITIS,\n",
    "    ACSC_DIABETES_COMPLICATIONS,\n",
    "    ACSC_DYSPEPSIA,\n",
    "    ACSC_ENT_INFECTION,\n",
    "    ACSC_HYPERTENSION,\n",
    "    ACSC_HYPOGLYCEMIA,\n",
    "    ACSC_HYPOKALEMIA,\n",
    "    ACSC_INFLUENZA_PNEUMONIA,\n",
    "    ACSC_MIGRAINE_HEADACHE,\n",
    "    ACSC_NUTRITION_DEFICIENT,\n",
    "    ACSC_PERFORATED_BLEEDING_ULCER,\n",
    "    ACSC_PROXIMAL_FEMUR_FRACTURE,\n",
    "    ACSC_PYELONEPHRITIS,\n",
    "    ACSC_UTI,\n",
    "    --ACSC_VACCINE_PREVENTABLE_DX,\n",
    "    DATEDIFF(year,DOB, TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01'))) as AGE,\n",
    "    AMB_ACSC_COST,\n",
    "    AMB_ACSC_COUNT,\n",
    "    BH__COUNT,\n",
    "    BH__SCORE,\n",
    "    BH_ALTERED_MENTAL_STATE,\n",
    "    BH_ALZHEIMERS_DEMENTIA,\n",
    "    BH_ANXIETY,\n",
    "    BH_BI_POLAR,\n",
    "    BH_DEPRESSION,\n",
    "    BH_SCHIZOPHRENIA,\n",
    "    BH_SUBABUSE,\n",
    "    CRN__COUNT,\n",
    "    CRN_AFIB,\n",
    "    CRN_ASTHMA,\n",
    "    CRN_CARDIOVASCULAR_DX,\n",
    "    CRN_CHRONIC_KIDNEY_DISEASE,\n",
    "    CRN_CONGESTIVE_HEART_FAILURE,\n",
    "    CRN_COPD,\n",
    "    CRN_DIABETES_W__ACUTE_COMP,\n",
    "    CRN_DIABETES_W__CHRONIC_COMP,\n",
    "    CRN_DIABETES_W_OUT_COMP,\n",
    "    CRN_FALLS,\n",
    "    CRN_GASTRO_ESOPH_REFLUX,\n",
    "    CRN_HIP_FRACTURE,\n",
    "    CRN_HTN,\n",
    "    CRN_OBESITY,\n",
    "    CRN_OSTEOPOROSIS,\n",
    "    CRN_PARKINSONS_DISEASE,\n",
    "    CRN_PRESSURE_ULCER,\n",
    "    CRN_PRIOR_MI,\n",
    "    CRN_PRIOR_STROKE,\n",
    "    CRN_SCORE,\n",
    "    CRN_SLEEP_APNEA,\n",
    "    CRN_SMOKING,\n",
    "    CRN_UTI,\n",
    "    DYAD_CKD_DD,\n",
    "    DYAD_CKD_OP,\n",
    "    DYAD_COPD_DD,\n",
    "    DYAD_COPD_HF,\n",
    "    DYAD_COPD_OP,\n",
    "    DYAD_COUNT,\n",
    "    DYAD_DM_CKD,\n",
    "    DYAD_DM_OP,\n",
    "    DYAD_HBP_HF,\n",
    "    DYAD_HF_CKD,\n",
    "    ED_ACSC_COST,\n",
    "    ED_ACSC_COUNT,\n",
    "    GENDER,\n",
    "    CASE WHEN \"GROUP\" = 'E' THEN 'A' ELSE \"GROUP\" END as \"GROUP\",\n",
    "    --HMKR_ACSC_COST,\n",
    "    --HMKR_ACSC_COUNT,\n",
    "    HTI_RISK_SCORE_V2_1,\n",
    "    IP_ACSC_COST,\n",
    "    IP_ACSC_COUNT,\n",
    "    IP_READMIT_ACSC_COST,\n",
    "    IP_READMIT_ACSC_COUNT,\n",
    "    IP_RHB_ACSC_COST,\n",
    "    IP_RHB_ACSC_COUNT,\n",
    "    CASE WHEN LANGUAGE_SPOKEN is NULL THEN 'Unknown'\n",
    "         WHEN LANGUAGE_SPOKEN = 'English' THEN 'English'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Chinese' THEN 'Chinese'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Spanish' THEN 'Spanish'\n",
    "         WHEN LANGUAGE_SPOKEN = 'Russian' THEN 'Russian'\n",
    "    ELSE 'Other' END AS LANGUAGE_SPOKEN_CLEAN,\n",
    "    --NI_COST_DENT,\n",
    "    NI_COST_ED,\n",
    "    NI_COST_HM,\n",
    "   --NI_COST_HMKR,\n",
    "    NI_COST_HS,\n",
    "    NI_COST_IP,\n",
    "    NI_COST_IP_RHB,\n",
    "    NI_COST_OP,\n",
    "    NI_COST_OTH,\n",
    "    --NI_COST_PCA_T1020,\n",
    "    --NI_COST_PCA_T1019,\n",
    "    NI_COST_PR,\n",
    "    NI_COST_PSYC,\n",
    "    NI_COST_RX,\n",
    "    --NI_COUNT_DENT,\n",
    "    NI_COUNT_ED,\n",
    "    NI_COUNT_HM,\n",
    "    --NI_COUNT_HMKR,\n",
    "    NI_COUNT_HS,\n",
    "    NI_COUNT_IP,\n",
    "    NI_COUNT_IP_RHB,\n",
    "    NI_COUNT_OP,\n",
    "    NI_COUNT_OTH,\n",
    "    --NI_COUNT_PCA_T1020,\n",
    "    --NI_COUNT_PCA_T1019,\n",
    "    NI_COUNT_PR,\n",
    "    NI_COUNT_PSYC,\n",
    "    NI_COUNT_RX,\n",
    "    NON_IMPACTABLE_CLAIM_COUNT,\n",
    "    OP_ACSC_COST,\n",
    "    OP_ACSC_COUNT,\n",
    "    --CAST(PART_C_RISK_SCORE as FLOAT) as PART_C_RISK_SCORE,\n",
    "    --PCA_T1020_ACSC_COUNT,\n",
    "    --PCA_T1020_ACSC_COST,\n",
    "    --PCA_T1019_ACSC_COUNT,\n",
    "    --PCA_T1019_ACSC_COST,\n",
    "    PR_ACSC_COST,\n",
    "    PR_ACSC_COUNT,\n",
    "    CASE WHEN RC is NULL THEN 'UNDEFINED' ELSE RC END AS RC_CLEAN,\n",
    "    SNF_COST,\n",
    "    SNF_COUNT,\n",
    "    TOTAL_IMPACTABLE_COST,\n",
    "    TOTAL_IMPACTABLE_COST_PRO,\n",
    "    TOTAL_NON_IMPACTABLE_COST\n",
    "FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" SCORE\n",
    "    LEFT JOIN EDIPTABLE\n",
    "        ON SCORE.MEMBER_ID = EDIPTABLE.MEMBER_ID\n",
    "            AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) = EDIPTABLE.DATE_START\n",
    "WHERE SCORE.CLNT = 'UHC_NE' //THIS NEEDS TO CHANGE BASED ON CLIENT\n",
    "    AND TO_DATE(CONCAT(LEFT(SCORE.DATA_DATE_START,4),'-',RIGHT(SCORE.DATA_DATE_START,2),'-01')) \n",
    "        = (SELECT max(TO_DATE(CONCAT(LEFT(DATA_DATE_START,4),'-',RIGHT(DATA_DATE_START,2),'-01'))) FROM \"VESTA_DEVELOPMENT\".\"ANALYST_SANDBOX\".\"CLNT_STRAT_VIP\" WHERE CLNT = 'UHC_NE') // Most Recent Stratification for Client// THIS NEEDS TO CHANGE BASED ON CLIENT'''\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # Dataframe creation\n",
    "    test_df = pd.DataFrame.from_records(iter(cursor), columns = [x[0] for x in cursor.description])\n",
    "    \n",
    "    print('Successful DataFrame Created')\n",
    "    \n",
    "    cursor.close\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close\n",
    "    \n",
    "print('Ready for Cleaning')  \n",
    "\n",
    "print(test_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2057 entries, 0 to 2056\n",
      "Columns: 118 entries, DATE_START to TOTAL_NON_IMPACTABLE_COST\n",
      "dtypes: float64(93), int64(2), object(23)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning Operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Remove Member ID and Data Start Date from Data Frame for prediction\n",
    "data_date_start_list = test_df['DATA_DATE_START']\n",
    "test_df = test_df.drop(['DATA_DATE_START'], axis = 1)\n",
    "member_id_list = test_df['MEMBER_ID']\n",
    "test_df = test_df.drop(['MEMBER_ID'], axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "\n",
    "#Remove ED_Visit from Data Frame\n",
    "y_test_df = test_df['ED_IP_VISIT']\n",
    "test_df = test_df.drop(['ED_IP_VISIT'], axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2057 entries, 0 to 2056\n",
      "Columns: 115 entries, ACSC__COUNT to TOTAL_NON_IMPACTABLE_COST\n",
      "dtypes: float64(93), int64(1), object(21)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features that initially had missing values within the data frame: \n",
      " ['ACSC_A_FIB_AND_FLUTTER', 'ACSC_ALCOHOL_RELATED', 'ACSC_ANEMIA', 'ACSC_ANGINA', 'ACSC_CONSTIPATION', 'ACSC_CONVULSION_EPILEPSY', 'ACSC_COPD', 'ACSC_ENT_INFECTION', 'ACSC_HYPOGLYCEMIA', 'ACSC_HYPOKALEMIA', 'ACSC_MIGRAINE_HEADACHE', 'ACSC_NUTRITION_DEFICIENT', 'ACSC_PROXIMAL_FEMUR_FRACTURE', 'CRN_PRIOR_MI', 'CRN_SMOKING', 'NI_COST_IP_RHB', 'NI_COUNT_IP_RHB'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find features with missing values\n",
    "sumdf = pd.DataFrame(test_df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "# Review features with missing values\n",
    "print('These are the features that initially had missing values within the data frame: \\n',features_to_drop,'\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "\n",
    "Nulls_to_correct =   ['ACSC_A_FIB_AND_FLUTTER', 'ACSC_ALCOHOL_RELATED', 'ACSC_ANEMIA', 'ACSC_ANGINA', 'ACSC_CONSTIPATION', 'ACSC_CONVULSION_EPILEPSY', 'ACSC_COPD', 'ACSC_ENT_INFECTION', 'ACSC_HYPOGLYCEMIA', 'ACSC_HYPOKALEMIA', 'ACSC_MIGRAINE_HEADACHE', 'ACSC_NUTRITION_DEFICIENT', 'ACSC_PROXIMAL_FEMUR_FRACTURE', 'CRN_PRIOR_MI', 'CRN_SMOKING', 'NI_COST_IP_RHB', 'NI_COUNT_IP_RHB']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These columns were dropped after because missing values were not corrected :\n",
      " [] \n",
      "\n",
      "\n",
      "These are the features that were converted to dummy variables: \n",
      " ['GENDER', 'GROUP', 'LANGUAGE_SPOKEN_CLEAN', 'RC_CLEAN'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fill selected features with 0 value\n",
    "for col in Nulls_to_correct:\n",
    "    test_df[col] = test_df[col].fillna(0)\n",
    "\n",
    "# Check for missing values and drop columns with missing values\n",
    "sumdf = pd.DataFrame(test_df.isna().sum())\n",
    "\n",
    "features_to_drop = []\n",
    "for row in sumdf.iterrows():\n",
    "    if row[1][0] != 0:\n",
    "        features_to_drop.append(row[0])\n",
    "\n",
    "print('These columns were dropped after because missing values were not corrected :\\n', features_to_drop,'\\n\\n')\n",
    "test_df = test_df.drop(columns = features_to_drop)\n",
    "\n",
    "# Convert object datatypes to dummy variables\n",
    "object_list = []\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtypes == 'object':\n",
    "        object_list.append(col)\n",
    "\n",
    "print('These are the features that were converted to dummy variables: \\n',object_list,'\\n\\n')\n",
    "test_df = pd.get_dummies(test_df, columns = object_list, drop_first = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2057 entries, 0 to 2056\n",
      "Columns: 115 entries, ACSC__COUNT to LANGUAGE_SPOKEN_CLEAN_Unknown\n",
      "dtypes: float64(93), int64(18), uint8(4)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2057 entries, 0 to 2056\n",
      "Columns: 115 entries, ACSC__COUNT to LANGUAGE_SPOKEN_CLEAN_Unknown\n",
      "dtypes: float64(93), int64(18), uint8(4)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "#Scale data for prediction with original model\n",
    "X = test_df\n",
    "X = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.9494621753692627 seconds ---\n",
      "--- 0.49962663650512695 seconds ---\n",
      "--- 0.516448974609375 seconds ---\n",
      "--- 0.5150835514068604 seconds ---\n",
      "--- 0.48209309577941895 seconds ---\n",
      "--- 0.484912633895874 seconds ---\n",
      "--- 0.4795815944671631 seconds ---\n",
      "--- 0.48669910430908203 seconds ---\n",
      "--- 0.47971677780151367 seconds ---\n",
      "--- 0.49661684036254883 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": "            1         2         3         4         5         6         7   \\\n0     0.217500  0.216000  0.242000  0.202500  0.236000  0.221000  0.283000   \n1     0.217500  0.317000  0.236500  0.291500  0.265000  0.263000  0.207500   \n2     0.237000  0.375500  0.291500  0.320500  0.309750  0.321163  0.256500   \n3     0.251500  0.249000  0.247000  0.208500  0.194500  0.246000  0.226500   \n4     0.183500  0.205000  0.148000  0.260000  0.181500  0.183000  0.217000   \n...        ...       ...       ...       ...       ...       ...       ...   \n2052  0.010000  0.023436  0.031160  0.027000  0.034214  0.052715  0.024697   \n2053  0.357500  0.299500  0.255500  0.365500  0.325000  0.342163  0.333500   \n2054  0.437573  0.377461  0.407638  0.389756  0.372443  0.378000  0.327000   \n2055  0.425500  0.438500  0.447250  0.524722  0.547000  0.464000  0.514500   \n2056  0.412500  0.437804  0.607500  0.593500  0.589500  0.656402  0.403189   \n\n            8         9         10  \n0     0.285000  0.255000  0.305000  \n1     0.240000  0.263500  0.271500  \n2     0.303395  0.381500  0.289000  \n3     0.225450  0.223000  0.217500  \n4     0.143000  0.197000  0.173500  \n...        ...       ...       ...  \n2052  0.063144  0.044662  0.033615  \n2053  0.378145  0.436187  0.341000  \n2054  0.353581  0.383873  0.425738  \n2055  0.425500  0.472801  0.467500  \n2056  0.452084  0.465000  0.624500  \n\n[2057 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.217500</td>\n      <td>0.216000</td>\n      <td>0.242000</td>\n      <td>0.202500</td>\n      <td>0.236000</td>\n      <td>0.221000</td>\n      <td>0.283000</td>\n      <td>0.285000</td>\n      <td>0.255000</td>\n      <td>0.305000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.217500</td>\n      <td>0.317000</td>\n      <td>0.236500</td>\n      <td>0.291500</td>\n      <td>0.265000</td>\n      <td>0.263000</td>\n      <td>0.207500</td>\n      <td>0.240000</td>\n      <td>0.263500</td>\n      <td>0.271500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.237000</td>\n      <td>0.375500</td>\n      <td>0.291500</td>\n      <td>0.320500</td>\n      <td>0.309750</td>\n      <td>0.321163</td>\n      <td>0.256500</td>\n      <td>0.303395</td>\n      <td>0.381500</td>\n      <td>0.289000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.251500</td>\n      <td>0.249000</td>\n      <td>0.247000</td>\n      <td>0.208500</td>\n      <td>0.194500</td>\n      <td>0.246000</td>\n      <td>0.226500</td>\n      <td>0.225450</td>\n      <td>0.223000</td>\n      <td>0.217500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.183500</td>\n      <td>0.205000</td>\n      <td>0.148000</td>\n      <td>0.260000</td>\n      <td>0.181500</td>\n      <td>0.183000</td>\n      <td>0.217000</td>\n      <td>0.143000</td>\n      <td>0.197000</td>\n      <td>0.173500</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2052</th>\n      <td>0.010000</td>\n      <td>0.023436</td>\n      <td>0.031160</td>\n      <td>0.027000</td>\n      <td>0.034214</td>\n      <td>0.052715</td>\n      <td>0.024697</td>\n      <td>0.063144</td>\n      <td>0.044662</td>\n      <td>0.033615</td>\n    </tr>\n    <tr>\n      <th>2053</th>\n      <td>0.357500</td>\n      <td>0.299500</td>\n      <td>0.255500</td>\n      <td>0.365500</td>\n      <td>0.325000</td>\n      <td>0.342163</td>\n      <td>0.333500</td>\n      <td>0.378145</td>\n      <td>0.436187</td>\n      <td>0.341000</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>0.437573</td>\n      <td>0.377461</td>\n      <td>0.407638</td>\n      <td>0.389756</td>\n      <td>0.372443</td>\n      <td>0.378000</td>\n      <td>0.327000</td>\n      <td>0.353581</td>\n      <td>0.383873</td>\n      <td>0.425738</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>0.425500</td>\n      <td>0.438500</td>\n      <td>0.447250</td>\n      <td>0.524722</td>\n      <td>0.547000</td>\n      <td>0.464000</td>\n      <td>0.514500</td>\n      <td>0.425500</td>\n      <td>0.472801</td>\n      <td>0.467500</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>0.412500</td>\n      <td>0.437804</td>\n      <td>0.607500</td>\n      <td>0.593500</td>\n      <td>0.589500</td>\n      <td>0.656402</td>\n      <td>0.403189</td>\n      <td>0.452084</td>\n      <td>0.465000</td>\n      <td>0.624500</td>\n    </tr>\n  </tbody>\n</table>\n<p>2057 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Data Frame to store results\n",
    "model_df = pd.DataFrame()\n",
    "\n",
    "for i,clf in forest_dict.items():\n",
    "    start_time = time.time()\n",
    "    np.random.seed(i)\n",
    "    model_df[i+1] = clf.predict_proba(X)[:,1]\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "model_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "             1         2         3         4         5         6         7  \\\n0     0.217500  0.216000  0.242000  0.202500  0.236000  0.221000  0.283000   \n1     0.217500  0.317000  0.236500  0.291500  0.265000  0.263000  0.207500   \n2     0.237000  0.375500  0.291500  0.320500  0.309750  0.321163  0.256500   \n3     0.251500  0.249000  0.247000  0.208500  0.194500  0.246000  0.226500   \n4     0.183500  0.205000  0.148000  0.260000  0.181500  0.183000  0.217000   \n...        ...       ...       ...       ...       ...       ...       ...   \n2052  0.010000  0.023436  0.031160  0.027000  0.034214  0.052715  0.024697   \n2053  0.357500  0.299500  0.255500  0.365500  0.325000  0.342163  0.333500   \n2054  0.437573  0.377461  0.407638  0.389756  0.372443  0.378000  0.327000   \n2055  0.425500  0.438500  0.447250  0.524722  0.547000  0.464000  0.514500   \n2056  0.412500  0.437804  0.607500  0.593500  0.589500  0.656402  0.403189   \n\n             8         9        10  VIP_SCORE  \n0     0.285000  0.255000  0.305000   0.246300  \n1     0.240000  0.263500  0.271500   0.257300  \n2     0.303395  0.381500  0.289000   0.308581  \n3     0.225450  0.223000  0.217500   0.228895  \n4     0.143000  0.197000  0.173500   0.189150  \n...        ...       ...       ...        ...  \n2052  0.063144  0.044662  0.033615   0.034464  \n2053  0.378145  0.436187  0.341000   0.343400  \n2054  0.353581  0.383873  0.425738   0.385306  \n2055  0.425500  0.472801  0.467500   0.472727  \n2056  0.452084  0.465000  0.624500   0.524198  \n\n[2057 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>VIP_SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.217500</td>\n      <td>0.216000</td>\n      <td>0.242000</td>\n      <td>0.202500</td>\n      <td>0.236000</td>\n      <td>0.221000</td>\n      <td>0.283000</td>\n      <td>0.285000</td>\n      <td>0.255000</td>\n      <td>0.305000</td>\n      <td>0.246300</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.217500</td>\n      <td>0.317000</td>\n      <td>0.236500</td>\n      <td>0.291500</td>\n      <td>0.265000</td>\n      <td>0.263000</td>\n      <td>0.207500</td>\n      <td>0.240000</td>\n      <td>0.263500</td>\n      <td>0.271500</td>\n      <td>0.257300</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.237000</td>\n      <td>0.375500</td>\n      <td>0.291500</td>\n      <td>0.320500</td>\n      <td>0.309750</td>\n      <td>0.321163</td>\n      <td>0.256500</td>\n      <td>0.303395</td>\n      <td>0.381500</td>\n      <td>0.289000</td>\n      <td>0.308581</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.251500</td>\n      <td>0.249000</td>\n      <td>0.247000</td>\n      <td>0.208500</td>\n      <td>0.194500</td>\n      <td>0.246000</td>\n      <td>0.226500</td>\n      <td>0.225450</td>\n      <td>0.223000</td>\n      <td>0.217500</td>\n      <td>0.228895</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.183500</td>\n      <td>0.205000</td>\n      <td>0.148000</td>\n      <td>0.260000</td>\n      <td>0.181500</td>\n      <td>0.183000</td>\n      <td>0.217000</td>\n      <td>0.143000</td>\n      <td>0.197000</td>\n      <td>0.173500</td>\n      <td>0.189150</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2052</th>\n      <td>0.010000</td>\n      <td>0.023436</td>\n      <td>0.031160</td>\n      <td>0.027000</td>\n      <td>0.034214</td>\n      <td>0.052715</td>\n      <td>0.024697</td>\n      <td>0.063144</td>\n      <td>0.044662</td>\n      <td>0.033615</td>\n      <td>0.034464</td>\n    </tr>\n    <tr>\n      <th>2053</th>\n      <td>0.357500</td>\n      <td>0.299500</td>\n      <td>0.255500</td>\n      <td>0.365500</td>\n      <td>0.325000</td>\n      <td>0.342163</td>\n      <td>0.333500</td>\n      <td>0.378145</td>\n      <td>0.436187</td>\n      <td>0.341000</td>\n      <td>0.343400</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>0.437573</td>\n      <td>0.377461</td>\n      <td>0.407638</td>\n      <td>0.389756</td>\n      <td>0.372443</td>\n      <td>0.378000</td>\n      <td>0.327000</td>\n      <td>0.353581</td>\n      <td>0.383873</td>\n      <td>0.425738</td>\n      <td>0.385306</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>0.425500</td>\n      <td>0.438500</td>\n      <td>0.447250</td>\n      <td>0.524722</td>\n      <td>0.547000</td>\n      <td>0.464000</td>\n      <td>0.514500</td>\n      <td>0.425500</td>\n      <td>0.472801</td>\n      <td>0.467500</td>\n      <td>0.472727</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>0.412500</td>\n      <td>0.437804</td>\n      <td>0.607500</td>\n      <td>0.593500</td>\n      <td>0.589500</td>\n      <td>0.656402</td>\n      <td>0.403189</td>\n      <td>0.452084</td>\n      <td>0.465000</td>\n      <td>0.624500</td>\n      <td>0.524198</td>\n    </tr>\n  </tbody>\n</table>\n<p>2057 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df['VIP_SCORE'] = (model_df[1] + model_df[2] + model_df[3] + model_df[4] + \n",
    "    model_df[5] +model_df[6] + model_df[7] + model_df[8] + model_df[9] + model_df[10])/10\n",
    "\n",
    "model_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "#Merge Results back to Test Data Frame\n",
    "test_df['VIP_SCORE'] = model_df['VIP_SCORE']\n",
    "test_df['ED_IP_VISIT'] = y_test_df\n",
    "test_df['MEMBER_ID'] = member_id_list\n",
    "test_df['DATA_DATE_START'] = data_date_start_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "       MEMBER_ID  VIP_SCORE  ED_IP_VISIT DATA_DATE_START\n863   3697395591   0.934910            0          202210\n1070  3113439652   0.933510            0          202210\n909     94048356   0.927260            0          202210\n788     89914722   0.924360            0          202210\n532     89923841   0.918450            0          202210\n931     90400484   0.907913            0          202210\n173     89862360   0.903450            0          202210\n1249    89866880   0.891238            0          202210\n1989    21035100   0.883344            0          202210\n894     89898655   0.880743            0          202210\n1575   105721627   0.876243            0          202210\n1994    94115349   0.873860            0          202210\n369     89897629   0.870202            0          202210\n549    760279075   0.869150            0          202210\n1738    89915545   0.868760            0          202210",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MEMBER_ID</th>\n      <th>VIP_SCORE</th>\n      <th>ED_IP_VISIT</th>\n      <th>DATA_DATE_START</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>863</th>\n      <td>3697395591</td>\n      <td>0.934910</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>3113439652</td>\n      <td>0.933510</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>909</th>\n      <td>94048356</td>\n      <td>0.927260</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>788</th>\n      <td>89914722</td>\n      <td>0.924360</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>532</th>\n      <td>89923841</td>\n      <td>0.918450</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>931</th>\n      <td>90400484</td>\n      <td>0.907913</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>89862360</td>\n      <td>0.903450</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>1249</th>\n      <td>89866880</td>\n      <td>0.891238</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>1989</th>\n      <td>21035100</td>\n      <td>0.883344</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>89898655</td>\n      <td>0.880743</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>1575</th>\n      <td>105721627</td>\n      <td>0.876243</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>1994</th>\n      <td>94115349</td>\n      <td>0.873860</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>369</th>\n      <td>89897629</td>\n      <td>0.870202</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>760279075</td>\n      <td>0.869150</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n    <tr>\n      <th>1738</th>\n      <td>89915545</td>\n      <td>0.868760</td>\n      <td>0</td>\n      <td>202210</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View Predicted Members DF\n",
    "predicted_df = test_df[['MEMBER_ID','VIP_SCORE','ED_IP_VISIT','DATA_DATE_START']]\n",
    "predicted_df = predicted_df.sort_values(by=['VIP_SCORE'],ascending = False)\n",
    "predicted_df.head(15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "766ae136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       MEMBER_ID  VIP_SCORE  ED_IP_VISIT DATA_DATE_START\n863   3697395591   0.934910            0      2022-10-01\n1070  3113439652   0.933510            0      2022-10-01\n909     94048356   0.927260            0      2022-10-01\n788     89914722   0.924360            0      2022-10-01\n532     89923841   0.918450            0      2022-10-01\n931     90400484   0.907913            0      2022-10-01\n173     89862360   0.903450            0      2022-10-01\n1249    89866880   0.891238            0      2022-10-01\n1989    21035100   0.883344            0      2022-10-01\n894     89898655   0.880743            0      2022-10-01\n1575   105721627   0.876243            0      2022-10-01\n1994    94115349   0.873860            0      2022-10-01\n369     89897629   0.870202            0      2022-10-01\n549    760279075   0.869150            0      2022-10-01\n1738    89915545   0.868760            0      2022-10-01",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MEMBER_ID</th>\n      <th>VIP_SCORE</th>\n      <th>ED_IP_VISIT</th>\n      <th>DATA_DATE_START</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>863</th>\n      <td>3697395591</td>\n      <td>0.934910</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>3113439652</td>\n      <td>0.933510</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>909</th>\n      <td>94048356</td>\n      <td>0.927260</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>788</th>\n      <td>89914722</td>\n      <td>0.924360</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>532</th>\n      <td>89923841</td>\n      <td>0.918450</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>931</th>\n      <td>90400484</td>\n      <td>0.907913</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>89862360</td>\n      <td>0.903450</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>1249</th>\n      <td>89866880</td>\n      <td>0.891238</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>1989</th>\n      <td>21035100</td>\n      <td>0.883344</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>89898655</td>\n      <td>0.880743</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>1575</th>\n      <td>105721627</td>\n      <td>0.876243</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>1994</th>\n      <td>94115349</td>\n      <td>0.873860</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>369</th>\n      <td>89897629</td>\n      <td>0.870202</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>760279075</td>\n      <td>0.869150</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n    <tr>\n      <th>1738</th>\n      <td>89915545</td>\n      <td>0.868760</td>\n      <td>0</td>\n      <td>2022-10-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View Predicted Members DF\n",
    "predicted_df = test_df[['MEMBER_ID','VIP_SCORE','ED_IP_VISIT','DATA_DATE_START']]\n",
    "predicted_df = predicted_df.sort_values(by=['VIP_SCORE'],ascending = False)\n",
    "predicted_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df.to_csv('UHC_NE_Model - 202209.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17587d",
   "metadata": {},
   "source": [
    "### Output and Update Tables in Snowflake for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e144d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Connection\n"
     ]
    }
   ],
   "source": [
    "# Snowflake credentials stored in environment variables\n",
    "\n",
    "username = os.getenv('Snowflake_User')\n",
    "password = os.getenv('Snowflake_password')\n",
    "account = os.getenv('Snowflake_account')\n",
    "\n",
    "#Define parameters if neccessary\n",
    "warehouse = 'DEVELOPER_STANDARD'\n",
    "database = 'VESTA_DEVELOPMENT'\n",
    "schema = 'ANALYST_SANDBOX'\n",
    "\n",
    "#Create connection object for Snowflake connection\n",
    "conn = sf.connect(user = username, password = password, account = account, warehouse = warehouse)\n",
    "\n",
    "#Execution function\n",
    "def execute_query(connection,query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close\n",
    "\n",
    "try:\n",
    "    sql = 'USE DATABASE {}'.format(database)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    sql = 'USE SCHEMA {}.{}'.format(database,schema)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    #Define warehouse to use in Snowflake\n",
    "    sql = 'use warehouse {}'.format(warehouse)\n",
    "    execute_query(conn,sql)\n",
    "    \n",
    "    print('Successful Connection')\n",
    "    \n",
    "    #Query to Snowflake\n",
    "    sql = \"CREATE TABLE IF NOT EXISTS VIP_SCORING (CLNT string,MEMBER_ID string, VIP_SCORE float ,DATA_DATE_START string)\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    entry_list = []\n",
    "    for row in predicted_df.iterrows():\n",
    "        client = 'UHC_NE'\n",
    "        member_id = str(row[1][0])\n",
    "        vip_score = row[1][1]\n",
    "        data_date = str(row[1][3])\n",
    "        entry = (client,member_id,vip_score,data_date)\n",
    "        entry_list.append(entry)       \n",
    "    entry = str(entry_list)[1:len(str(entry_list))-1]\n",
    "    sql = 'INSERT INTO VIP_SCORING (CLNT,MEMBER_ID, VIP_SCORE, DATA_DATE_START) VALUES {}'.format(entry)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)  \n",
    "    cursor.close\n",
    "    \n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "finally:\n",
    "    conn.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88077bed",
   "metadata": {},
   "source": [
    "# ITEMS BELOW THIS ARE FOR ANALYSIS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1182c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction List\n",
    "for row in predicted_df.iterrows():\n",
    "    print('Member: ',row[1][0],' VIP_SCORE: ',round(row[1][1],4), ' ED_IP_VISIT: ',row[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc46052",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_threshold = 0.3\n",
    "\n",
    "final_y_pred = [1 if result >= final_threshold else 0 for result in test_df['VIP_SCORE'].tolist()]\n",
    "\n",
    "# Creating confusion maxtrix\n",
    "new_cnf_matrix = metrics.confusion_matrix(y_test_df,final_y_pred)\n",
    "\n",
    "%matplotlib inline\n",
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(new_cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# Axis labels\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#Calulating Metrics\n",
    "accuracy = metrics.accuracy_score(y_test_df, final_y_pred)\n",
    "precision = metrics.precision_score(y_test_df, final_y_pred)\n",
    "recall = metrics.recall_score(y_test_df, final_y_pred)\n",
    "    \n",
    "print('Thereshold: ',final_threshold)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realtime Accuracy\n",
    "X = []\n",
    "Y = []\n",
    "count = 1\n",
    "sums = 0\n",
    "for row in predicted_df.iterrows():\n",
    "    sums += row[1][2]\n",
    "    accuracy = round(sums/count *100,2)\n",
    "    X.append(count)\n",
    "    Y.append(accuracy)\n",
    "    print('After',count, 'prediction, the realtime accuarcy is ',accuracy)\n",
    "    count +=1\n",
    "    \n",
    "plt.plot(X, Y, marker='.', label='Random Forest')\n",
    "\n",
    "acc = np.where(np.array(Y) <= 60)\n",
    "\n",
    "acc = np.where(np.array(Y) <= 60)\n",
    "#for i,x in enumerate(acc[0]):\n",
    "    #print(i,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71bfc5",
   "metadata": {},
   "source": [
    "### Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3856ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [x for x in df.columns if x != 'ED_IP_VISIT']\n",
    "\n",
    "importance_df = pd.DataFrame()\n",
    "\n",
    "for i,clf in forest_dict.items():\n",
    "    start_time = time.time()\n",
    "    importances = list(clf.feature_importances_)\n",
    "    feature_importances = [(feature, round(importance, 15)) for feature, importance in zip(feature_list, importances)]\n",
    "    importance_df[i+1] = feature_importances\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "importance_avg_list = []\n",
    "\n",
    "for row in importance_df.iterrows():\n",
    "    feature_name = row[1][1][0]\n",
    "    avg = round((row[1][1][1] + row[1][2][1] + row[1][3][1] + row[1][4][1] + row[1][5][1] + row[1][6][1] + row[1][7][1] \n",
    "            + row[1][8][1] + row[1][9][1] + row[1][10][1])/10,4)\n",
    "    \n",
    "    importance_avg_list.append((feature_name,avg))\n",
    "    \n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(importance_avg_list, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "feature_importances[0:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb55d49",
   "metadata": {},
   "source": [
    "### Random Search with Cross Validation - Hyperparameter Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(random_state = 22822)\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scoring = {\"Accuracy\":make_scorer(metrics.accuracy_score),\"Precision\":make_scorer(metrics.precision_score),\n",
    "            \"Recall\":make_scorer(metrics.recall_score),\"AUC\":make_scorer(metrics.roc_auc_score)}\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='balanced_accuracy', \n",
    "                              cv = 3, verbose=6, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train,y_train);\n",
    "\n",
    "#Best parameters found\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f54ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.cv_results_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
